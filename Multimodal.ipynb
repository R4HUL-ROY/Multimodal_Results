{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1x6_cP5LupRYKPcrpBbUdgIINKmIOuoZ0",
      "authorship_tag": "ABX9TyPn/s+Xk59HWyM8y5lM/4qQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R4HUL-ROY/Multimodal_Results/blob/main/Multimodal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-snMEQ4G0Ke",
        "outputId": "843d5fea-3b9e-4457-fa90-7029bd87b615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.0%2Bzzzcolab20220506162203-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[K     - 668.3 MB 11.2 MB/s\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.46.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.0.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.21.6)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.26.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed tensorflow-2.8.0+zzzcolab20220506162203 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 43 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 7s (62.5 MB/s)\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155610 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.8\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q tensorflow_text\n",
        "!pip install fasttext\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import fasttext.util\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "# import tensorflow_text as text\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam, Adamax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ch6N2R2G7uX",
        "outputId": "dfc4f680-2b6e-4f5c-9c45-16b39fcfb772"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3145252 sha256=67504caf4084486f2249c13c7631d2210dd60dd8a0e70f409db848e3ab1157b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_dataset_path = \"/content/drive/MyDrive/project_resources/merged_data_for_multimodal_model.csv\"\n",
        "stopwords_path = \"/content/drive/MyDrive/project_resources/stopwords.txt\"\n",
        "glove_vector_path = '/content/drive/MyDrive/project_resources/glove.6B.200d.txt'\n",
        "fasttext_model_path = 'cc.en.300.bin'\n",
        "\n",
        "vgg16_imagenet_weight_path = \"/content/drive/MyDrive/project_resources/Imagenet_weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "resnet50_imagenet_weight_path = \"/content/drive/MyDrive/project_resources/Imagenet_weights/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "mobilenetv2_imagenet_weight_path = \"/content/drive/MyDrive/project_resources/Imagenet_weights/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\""
      ],
      "metadata": {
        "id": "q0i8Eg_3XTZj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the data"
      ],
      "metadata": {
        "id": "2ITSwycg8AzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img_data_root = pathlib.Path('/content/drive/MyDrive/Tobacco3482-jpg/')\n",
        "# print(img_data_root)\n",
        "# for item in img_data_root.iterdir():\n",
        "#   print(item)\n",
        "\n",
        "# print()\n",
        "\n",
        "# text_data_root = pathlib.Path('/content/drive/MyDrive/tobaco_OCR/')\n",
        "# print(text_data_root)\n",
        "# for item in text_data_root.iterdir():\n",
        "#   print(item)"
      ],
      "metadata": {
        "id": "QemE440Q711U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_corresponding_txtpath(img_path):\n",
        "#     return img_path.replace(\"Tobacco3482-jpg\", \"tobaco_OCR\")[:-3] + \"txt\"\n",
        "    \n",
        "\n",
        "# def get_file_paths_and_labels(img_data_root, text_data_root):\n",
        "#      img_paths = [str(path) for path in img_data_root.glob('*/*.jpg')]\n",
        "#      text_paths = [get_corresponding_txtpath(this_path) for this_path in img_paths]\n",
        "#      img_labels = [p.split(\"/\")[-2] for p in img_paths]\n",
        "#      text_labels = [p.split(\"/\")[-2] for p in text_paths]\n",
        "#      return img_paths, img_labels, text_paths, text_labels\n",
        "\n",
        "# img_paths, img_labels, text_paths, text_labels = get_file_paths_and_labels(img_data_root, text_data_root)\n",
        "# print(len(img_paths))\n",
        "# print(len(img_labels))\n",
        "# print(len(text_paths))\n",
        "# print(len(text_labels))"
      ],
      "metadata": {
        "id": "3hcVePal713n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_text_from_path(path):\n",
        "#     with open(path) as f:\n",
        "#         lines = f.readlines()\n",
        "#         lines  = ' '.join(lines)\n",
        "#         f.close()\n",
        "#     return lines\n",
        "\n",
        "# out_text = get_text_from_path('/content/drive/MyDrive/tobaco_OCR/ADVE/0000435350.txt') \n",
        "# print(out_text)"
      ],
      "metadata": {
        "id": "62pTUy397157"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_contains = []\n",
        "# c= 0 \n",
        "# for i, this_path in enumerate(text_paths):\n",
        "#     text_contains.append(get_text_from_path(this_path))\n",
        "#     print(c, end= \" \")\n",
        "#     c +=1"
      ],
      "metadata": {
        "id": "14f57wf1718Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.DataFrame(list(zip(text_paths, text_contains, img_paths, img_labels)),\n",
        "#                columns =['text_paths','texts', 'img_paths', 'data_label'])\n",
        "\n",
        "# # Merged Dataframe\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "yv6B8lgM71-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv(\"merged_data_for_multimodal_model.csv\")"
      ],
      "metadata": {
        "id": "O6RWknWGtKZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(merged_dataset_path)\n",
        "print(len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "EQkVjvVIG7VZ",
        "outputId": "18903e45-093b-4fa5-fb15-65f021c5a20e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3482\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          text_paths  \\\n",
              "0  /content/drive/MyDrive/tobaco_OCR/ADVE/0000136...   \n",
              "1  /content/drive/MyDrive/tobaco_OCR/ADVE/0000435...   \n",
              "2  /content/drive/MyDrive/tobaco_OCR/ADVE/0030049...   \n",
              "3  /content/drive/MyDrive/tobaco_OCR/ADVE/0000556...   \n",
              "4  /content/drive/MyDrive/tobaco_OCR/ADVE/0349627...   \n",
              "\n",
              "                                               texts  \\\n",
              "0  \\n \\n \\n \\n A Mpertant as yar\\n sesiye teaeter...   \n",
              "1  \\n TE che fitm\\n m66400 7127\\n KOOLS are the o...   \n",
              "2  SR Onrel ules cee\\n Nee dss\\n The one tales WT...   \n",
              "3  so ARN Rr nr\\n BWR Ga ||\\n Vending Operators\\n...   \n",
              "4  \\n &\\n BR. :\\n er non\\n be 4\\n op Re eo eee ee...   \n",
              "\n",
              "                                           img_paths data_label  \n",
              "0  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...       ADVE  \n",
              "1  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...       ADVE  \n",
              "2  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...       ADVE  \n",
              "3  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...       ADVE  \n",
              "4  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/03...       ADVE  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f907682-270a-4cb3-b4f1-2eae58fdefcc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_paths</th>\n",
              "      <th>texts</th>\n",
              "      <th>img_paths</th>\n",
              "      <th>data_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000136...</td>\n",
              "      <td>\\n \\n \\n \\n A Mpertant as yar\\n sesiye teaeter...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>ADVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000435...</td>\n",
              "      <td>\\n TE che fitm\\n m66400 7127\\n KOOLS are the o...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>ADVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0030049...</td>\n",
              "      <td>SR Onrel ules cee\\n Nee dss\\n The one tales WT...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>ADVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000556...</td>\n",
              "      <td>so ARN Rr nr\\n BWR Ga ||\\n Vending Operators\\n...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>ADVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0349627...</td>\n",
              "      <td>\\n &amp;\\n BR. :\\n er non\\n be 4\\n op Re eo eee ee...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/03...</td>\n",
              "      <td>ADVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f907682-270a-4cb3-b4f1-2eae58fdefcc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f907682-270a-4cb3-b4f1-2eae58fdefcc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f907682-270a-4cb3-b4f1-2eae58fdefcc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['data_label']= le.fit_transform(df['data_label'])\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CvyR128bG7Xf",
        "outputId": "dc1bd311-4c71-47d6-e319-9ec1f03e116b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          text_paths  \\\n",
              "0  /content/drive/MyDrive/tobaco_OCR/ADVE/0000136...   \n",
              "1  /content/drive/MyDrive/tobaco_OCR/ADVE/0000435...   \n",
              "2  /content/drive/MyDrive/tobaco_OCR/ADVE/0030049...   \n",
              "3  /content/drive/MyDrive/tobaco_OCR/ADVE/0000556...   \n",
              "4  /content/drive/MyDrive/tobaco_OCR/ADVE/0349627...   \n",
              "\n",
              "                                               texts  \\\n",
              "0  \\n \\n \\n \\n A Mpertant as yar\\n sesiye teaeter...   \n",
              "1  \\n TE che fitm\\n m66400 7127\\n KOOLS are the o...   \n",
              "2  SR Onrel ules cee\\n Nee dss\\n The one tales WT...   \n",
              "3  so ARN Rr nr\\n BWR Ga ||\\n Vending Operators\\n...   \n",
              "4  \\n &\\n BR. :\\n er non\\n be 4\\n op Re eo eee ee...   \n",
              "\n",
              "                                           img_paths  data_label  \n",
              "0  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "1  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "2  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "3  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "4  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/03...           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f0f87971-b379-4d3d-bb6a-42eb145f635d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_paths</th>\n",
              "      <th>texts</th>\n",
              "      <th>img_paths</th>\n",
              "      <th>data_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000136...</td>\n",
              "      <td>\\n \\n \\n \\n A Mpertant as yar\\n sesiye teaeter...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000435...</td>\n",
              "      <td>\\n TE che fitm\\n m66400 7127\\n KOOLS are the o...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0030049...</td>\n",
              "      <td>SR Onrel ules cee\\n Nee dss\\n The one tales WT...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000556...</td>\n",
              "      <td>so ARN Rr nr\\n BWR Ga ||\\n Vending Operators\\n...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0349627...</td>\n",
              "      <td>\\n &amp;\\n BR. :\\n er non\\n be 4\\n op Re eo eee ee...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/03...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f0f87971-b379-4d3d-bb6a-42eb145f635d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f0f87971-b379-4d3d-bb6a-42eb145f635d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f0f87971-b379-4d3d-bb6a-42eb145f635d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {}\n",
        "for idx, row in enumerate(df['text_paths']):\n",
        "    path = df.at[idx, 'text_paths']\n",
        "    lab = path.split(\"/\")[-2]\n",
        "    label_dict[str(lab)] = df.at[idx, 'data_label']\n",
        "    \n",
        "print(label_dict) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB8KrpAKG7Zi",
        "outputId": "3221c716-f721-449e-aa6f-dae2f0b80e64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ADVE': 0, 'Form': 2, 'Note': 6, 'Email': 1, 'News': 5, 'Resume': 8, 'Scientific': 9, 'Memo': 4, 'Report': 7, 'Letter': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess(text_string):\n",
        "    preprocessed_string = re.sub(r'[^\\w\\s]','',text_string)\n",
        "    preprocessed_string = preprocessed_string.replace('\\n',' ')\n",
        "    preprocessed_string = preprocessed_string.replace('_',' ')\n",
        "    preprocessed_string = re.sub(' +', ' ', preprocessed_string)\n",
        "    return preprocessed_string"
      ],
      "metadata": {
        "id": "HMeFaH8EG7bt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize, Lemmatize, stopwords removal\n",
        "import spacy \n",
        "import nltk\n",
        "# nlp = spacy.load(\"en\", disable=['parser', 'tagger', 'ner'])\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "# stops = stopwords.words(\"english\")\n",
        "\n",
        "def get_stopwords(file_path):\n",
        "    with open(file_path, \"r\") as fp:\n",
        "        content = fp.read()\n",
        "        stops = content.split(\"\\n\")\n",
        "        stops = stops[:-1]\n",
        "        fp.close()\n",
        "        return stops\n",
        "\n",
        "stops = get_stopwords(stopwords_path)\n",
        "def normalize(comment, lowercase, remove_stopwords):\n",
        "    if lowercase:\n",
        "        comment = comment.lower()\n",
        "    comment = nlp(comment)\n",
        "    lemmatized = list()\n",
        "    for word in comment:\n",
        "        lemma = word.lemma_.strip()\n",
        "        if lemma:\n",
        "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
        "                lemmatized.append(lemma)\n",
        "    return \" \".join(lemmatized)\n",
        "\n",
        "normalize(\"counting playing the Home\", lowercase=True, remove_stopwords=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lNwB2OC8G7fm",
        "outputId": "1e218c65-eb66-4b58-cc80-4e55df552313"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'count play home'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['texts'] = [preprocess(str(this_text)) for this_text in df['texts']]\n",
        "df['texts'] = [normalize(this_text, lowercase=True, remove_stopwords=True) for this_text in df['texts']]\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bHN3LSknG7hZ",
        "outputId": "d19e0e84-55c0-4660-f8c0-369d98da1a8a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          text_paths  \\\n",
              "0  /content/drive/MyDrive/tobaco_OCR/ADVE/0000136...   \n",
              "1  /content/drive/MyDrive/tobaco_OCR/ADVE/0000435...   \n",
              "2  /content/drive/MyDrive/tobaco_OCR/ADVE/0030049...   \n",
              "3  /content/drive/MyDrive/tobaco_OCR/ADVE/0000556...   \n",
              "4  /content/drive/MyDrive/tobaco_OCR/ADVE/0349627...   \n",
              "\n",
              "                                               texts  \\\n",
              "0  mpertant yar sesiye teaetere cabiieess baely k...   \n",
              "1  te che fitm m66400 7127 kool cigarette taste g...   \n",
              "2  sr onrel ules cee nee dss one tale wt lower ta...   \n",
              "3  arn rr nr bwr ga vend operator column worth 8 ...   \n",
              "4  br er non 4 op eo eee ee eee talk smoking deci...   \n",
              "\n",
              "                                           img_paths  data_label  \n",
              "0  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "1  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "2  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "3  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...           0  \n",
              "4  /content/drive/MyDrive/Tobacco3482-jpg/ADVE/03...           0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0890b20-9f2f-4866-98e7-2fa6f256338a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_paths</th>\n",
              "      <th>texts</th>\n",
              "      <th>img_paths</th>\n",
              "      <th>data_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000136...</td>\n",
              "      <td>mpertant yar sesiye teaetere cabiieess baely k...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000435...</td>\n",
              "      <td>te che fitm m66400 7127 kool cigarette taste g...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0030049...</td>\n",
              "      <td>sr onrel ules cee nee dss one tale wt lower ta...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0000556...</td>\n",
              "      <td>arn rr nr bwr ga vend operator column worth 8 ...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/00...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/tobaco_OCR/ADVE/0349627...</td>\n",
              "      <td>br er non 4 op eo eee ee eee talk smoking deci...</td>\n",
              "      <td>/content/drive/MyDrive/Tobacco3482-jpg/ADVE/03...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0890b20-9f2f-4866-98e7-2fa6f256338a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d0890b20-9f2f-4866-98e7-2fa6f256338a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d0890b20-9f2f-4866-98e7-2fa6f256338a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "LcweAJ8pHmol"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "0xyXKSByG7k7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 500\n",
        "# Fit the tokenizer\n",
        "tokenizer = Tokenizer(num_words=65000)\n",
        "tokenizer.fit_on_texts(train_df['texts'])\n",
        "\n",
        "# sequence the input corpus and add zero padding upto 500 word\n",
        "train_sequence = tokenizer.texts_to_sequences(train_df['texts'])\n",
        "train_padded = pad_sequences(train_sequence, maxlen = max_len, truncating = \"post\", padding = \"post\" )\n",
        "\n",
        "valid_sequence = tokenizer.texts_to_sequences(val_df['texts'])\n",
        "valid_padded = pad_sequences(valid_sequence, maxlen = max_len, truncating = \"post\", padding = \"post\" )\n",
        "\n",
        "test_sequence = tokenizer.texts_to_sequences(test_df['texts'])\n",
        "test_padded = pad_sequences(test_sequence, maxlen = max_len, truncating = \"post\", padding = \"post\" )\n",
        "\n",
        "train_tensor = [tf.convert_to_tensor(train_padded[i]) for i in range(train_padded.shape[0])]\n",
        "train_df['texts_embedding'] = train_tensor\n",
        "\n",
        "test_tensor = [tf.convert_to_tensor(test_padded[i]) for i in range(test_padded.shape[0])]\n",
        "test_df['texts_embedding'] = test_tensor   \n",
        "\n",
        "val_tensor = [tf.convert_to_tensor(valid_padded[i]) for i in range(valid_padded.shape[0])]\n",
        "val_df['texts_embedding'] = val_tensor\n"
      ],
      "metadata": {
        "id": "e7f-wIQUIv9X"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "num_words = len(word_index) + 1"
      ],
      "metadata": {
        "id": "sLGVE198hWXM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_embedding_matrix(text_model):\n",
        "    if text_model == \"glove\":\n",
        "        emmbed_dict = {}\n",
        "        with open(glove_vector_path ,'r') as f:\n",
        "            for line in f:\n",
        "                values = line.split()\n",
        "                word = values[0]\n",
        "                vector = np.asarray(values[1:],'float32')\n",
        "                emmbed_dict[word]=vector\n",
        "            f.close()\n",
        "        embedding_matrix = np.zeros((num_words, 200))\n",
        "        for word, i in word_index.items():\n",
        "            if i < num_words:\n",
        "                emb_vec = emmbed_dict.get(word)\n",
        "                if emb_vec is not None:\n",
        "                    embedding_matrix[i] = emb_vec\n",
        "        return embedding_matrix \n",
        "\n",
        "    elif text_model == \"fasttext\":  \n",
        "        ft = fasttext.load_model(fasttext_model_path)    \n",
        "        embedding_matrix = np.zeros((num_words, 300)) \n",
        "        for word, i in word_index.items():\n",
        "            if i < num_words:\n",
        "                emb_vec = ft.get_word_vector(word)\n",
        "                if emb_vec is not None:\n",
        "                    embedding_matrix[i] = emb_vec \n",
        "        return embedding_matrix    \n",
        "    return None                                       "
      ],
      "metadata": {
        "id": "pljgtsK9In4l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe_to_dataset(dataframe):\n",
        "    d = {}\n",
        "    embed_tensor = []\n",
        "    for i in dataframe['texts_embedding']:\n",
        "        embed_tensor.append(tf.convert_to_tensor(i))\n",
        "\n",
        "    img_path_tensor= []\n",
        "    for i in dataframe['img_paths']:\n",
        "        img_path_tensor.append(tf.convert_to_tensor(i))\n",
        "\n",
        "    d['texts_embedding'] = embed_tensor\n",
        "    d['img_paths'] = img_path_tensor\n",
        "\n",
        "    labels = dataframe[\"data_label\"]\n",
        "    labels = tf.convert_to_tensor(labels)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((d, labels))\n",
        "    return ds"
      ],
      "metadata": {
        "id": "6VoNrI5cG7m0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def preprocess_image(image_path):\n",
        "    extension = tf.strings.split(image_path)[-1]\n",
        "\n",
        "    image = tf.io.read_file(image_path)\n",
        "    if extension == b\"jpg\":\n",
        "        image = tf.image.decode_jpeg(image, 3)\n",
        "    else:\n",
        "        image = tf.image.decode_png(image, 3)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "    image = tf.image.per_image_standardization(image)\n",
        "    return image\n",
        "\n",
        "@tf.function\n",
        "def preprocess_text(sample):\n",
        "    pass\n",
        "\n",
        "@tf.function\n",
        "def preprocess_text_and_image(sample):\n",
        "    image = preprocess_image(sample[\"img_paths\"])\n",
        "    text = sample['texts_embedding']\n",
        "    return {\"image_inputs\": image,  \"text_inputs\": text}"
      ],
      "metadata": {
        "id": "FPkfitFvG7o0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "auto = tf.data.AUTOTUNE\n",
        "\n",
        "def prepare_dataset(dataframe, training = True):\n",
        "    ds = dataframe_to_dataset(dataframe)\n",
        "    if training:\n",
        "        ds = ds.shuffle(len(train_df))\n",
        "    ds = ds.map(lambda x, y: (preprocess_text_and_image(x), y)).cache()\n",
        "    ds = ds.batch(batch_size).prefetch(auto)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "J88nwUvfG7qp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = prepare_dataset(train_df)\n",
        "val_ds = prepare_dataset(val_df, False)\n",
        "test_ds = prepare_dataset(test_df, False)"
      ],
      "metadata": {
        "id": "Vc7keUxPHmq1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Flatten,  Dense, Dropout, Conv1D, GlobalMaxPooling1D, MaxPooling1D, GlobalMaxPooling2D\n",
        "from keras.initializers import Constant\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "aZHvW1L2HmtB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_encoder(text_projection_dims, dropout_rate, text_model, text_model_trainable):\n",
        "\n",
        "    embedding_matrix = get_embedding_matrix(text_model)\n",
        "\n",
        "    filter_size = 128\n",
        "    stride_unit = 1\n",
        "    kernels = 3\n",
        "    dropout_rate = 0.2\n",
        "    dropout_rate_conv = 0.5\n",
        "\n",
        "    inputs = keras.Input(shape=(500,), dtype=tf.int32, name=\"text_inputs\")\n",
        "\n",
        "    if text_model == \"glove\":\n",
        "        embed = tf.keras.layers.Embedding(  num_words,\n",
        "                                            200,\n",
        "                                            input_length = max_len,\n",
        "                                            embeddings_initializer = Constant(embedding_matrix),\n",
        "                                            trainable = text_model_trainable\n",
        "                                        )(inputs)\n",
        "    if text_model == \"fasttext\":\n",
        "        embed = tf.keras.layers.Embedding(  num_words,\n",
        "                                            300,\n",
        "                                            input_length = max_len,\n",
        "                                            embeddings_initializer = Constant(embedding_matrix),\n",
        "                                            trainable = text_model_trainable\n",
        "                                        )(inputs)\n",
        "\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(embed)\n",
        "\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=filter_size,kernel_size=kernels, padding='valid',activation='relu',strides=stride_unit,\n",
        "                               kernel_initializer = \"glorot_uniform\" ,bias_initializer = 'zeros')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate_conv)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=filter_size,kernel_size=kernels, padding='valid',activation='relu',strides=stride_unit,\n",
        "                               kernel_initializer = \"glorot_uniform\" ,bias_initializer = 'zeros')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate_conv)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv1D(filters=filter_size,kernel_size=kernels, padding='valid',activation='relu',strides=stride_unit,\n",
        "                               kernel_initializer = \"glorot_uniform\" ,bias_initializer = 'zeros')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate_conv)(x)\n",
        "    \n",
        "    x = tf.keras.layers.Conv1D(filters=filter_size,kernel_size=kernels, padding='valid',activation='relu',strides=stride_unit,\n",
        "                               kernel_initializer = \"glorot_uniform\" ,bias_initializer = 'zeros')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate_conv)(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    # x = tf.math.l2_normalize(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate_conv)(x)\n",
        "     \n",
        "    outputs = keras.layers.Dense(units=text_projection_dims,activation=\"relu\",kernel_initializer=\"glorot_uniform\",bias_initializer='zeros')(x)\n",
        "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
      ],
      "metadata": {
        "id": "ohTkuTPRHmvI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vision_encoder(img_projection_dims, dropout_rate, image_model, img_base_model_trainable):\n",
        "    img_shape=(224,224,3)\n",
        "    seed_value = 42\n",
        "\n",
        "\n",
        "    if image_model == \"vgg16\":\n",
        "        base_model = tf.keras.applications.vgg16.VGG16(include_top=False,input_shape=img_shape,pooling='avg',classes=10,weights=None)\n",
        "        base_model.load_weights(vgg16_imagenet_weight_path)\n",
        "        base_model.trainable = img_base_model_trainable\n",
        "\n",
        "    elif image_model == \"resnet50\":\n",
        "        base_model= tf.keras.applications.ResNet50(include_top=False,input_shape=img_shape,pooling='avg',classes=10,weights=None)\n",
        "        base_model.load_weights(resnet50_imagenet_weight_path)\n",
        "        base_model.trainable = img_base_model_trainable\n",
        "\n",
        "    elif image_model == \"mobilenetv2\":\n",
        "        base_model=tf.keras.applications.MobileNetV2(include_top=False,input_shape=img_shape,pooling='avg', classes=10,weights=None)\n",
        "        base_model.load_weights(mobilenetv2_imagenet_weight_path)\n",
        "        base_model.trainable = img_base_model_trainable\n",
        "\n",
        "\n",
        "\n",
        "    inputs = keras.Input(shape=img_shape)\n",
        "    x = base_model(inputs, training = img_base_model_trainable)\n",
        "    outputs =tf.keras.layers.Dense(img_projection_dims,kernel_initializer =\"glorot_uniform\",bias_initializer= \"zeros\",activation='relu')(x)\n",
        "\n",
        "    model=Model(inputs, outputs)\n",
        "\n",
        "    # Receive the images as inputs.\n",
        "    image_inputs = keras.Input(shape=(224, 224, 3), name=\"image_inputs\")\n",
        "\n",
        "    if image_model == \"vgg16\":\n",
        "        preprocessed_image = tf.keras.applications.vgg16.preprocess_input(image_inputs)\n",
        "    if image_model == \"resnet50\":\n",
        "        preprocessed_image = tf.keras.applications.resnet50.preprocess_input(image_inputs)    \n",
        "    if image_model == \"mobilenetv2\":\n",
        "        preprocessed_image = tf.keras.applications.mobilenet_v2.preprocess_input(image_inputs)\n",
        "\n",
        "    embeddings = model(preprocessed_image)\n",
        "\n",
        "    # Create the vision encoder model.\n",
        "    return keras.Model(image_inputs, embeddings, name=\"vision_encoder\")"
      ],
      "metadata": {
        "id": "YUavauzSHmxE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multimodal_model(**kwargs):\n",
        "\n",
        "    img_projection_dims = kwargs['img_projection_dims']\n",
        "    text_projection_dims = kwargs['text_projection_dims']\n",
        "    dropout_rate = kwargs['dropout_rate']\n",
        "    image_model = kwargs['image_model']\n",
        "    text_model = kwargs['text_model']\n",
        "    img_base_model_trainable = kwargs['img_base_model_trainable']\n",
        "    text_model_trainable = kwargs['text_model_trainable']\n",
        "\n",
        "    if image_model == \"None\" and text_model == \"None\":\n",
        "        print(\"Both the image_model and text_model cannot be None at the same time !!\")\n",
        "        return\n",
        "    # Receive the images and text as inputs.\n",
        "    if image_model != \"None\":\n",
        "        image_inputs = keras.Input(shape=(224, 224, 3), name=\"image_inputs\")\n",
        "        vision_encoder = create_vision_encoder(img_projection_dims, dropout_rate, image_model, img_base_model_trainable)\n",
        "        vision_projections = vision_encoder(image_inputs)\n",
        "\n",
        "    if text_model != \"None\":   \n",
        "        text_inputs = keras.Input(shape=(500,), dtype=tf.int32, name=\"text_inputs\")\n",
        "        text_encoder = create_text_encoder(text_projection_dims, dropout_rate, text_model, text_model_trainable)\n",
        "        text_projections = text_encoder(text_inputs)\n",
        "\n",
        "\n",
        "    # Concatenate the projections and pass through the classification layer.\n",
        "    if image_model != \"None\" and text_model != \"None\":\n",
        "        concatenated = keras.layers.Concatenate()([vision_projections, text_projections])\n",
        "    # concatenated = tf.keras.layers.BatchNormalization(momentum=0.9)(concatenated)\n",
        "\n",
        "\n",
        "    if image_model == \"None\" and text_model != \"None\":\n",
        "        outputs = keras.layers.Dense(10, activation=\"softmax\")(text_projections)\n",
        "        return keras.Model(text_inputs, outputs)\n",
        "    elif text_model == \"None\" and image_model != \"None\":    \n",
        "        outputs = keras.layers.Dense(10, activation=\"softmax\")(vision_projections)\n",
        "        return keras.Model(image_inputs, outputs)\n",
        "    elif text_model != \"None\" and image_model != \"None\":\n",
        "        outputs = keras.layers.Dense(10, activation=\"softmax\")(concatenated)\n",
        "        return keras.Model([image_inputs, text_inputs], outputs)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "bSJ8B8tqHmzA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "options for image_model : [\"vgg16\", \"resnet50\", \"mobilenetv2\"] (case-sensitive input)\n",
        "options for text_model : [\"glove\", \"fasttext\"] (case-sensitive input)\n",
        "\"\"\"\n",
        "\n",
        "multimodal_model = create_multimodal_model( \n",
        "    img_projection_dims=512,\n",
        "    text_projection_dims=512,\n",
        "    dropout_rate=0.2,\n",
        "    image_model = \"vgg16\",\n",
        "    text_model = \"None\", \n",
        "    img_base_model_trainable=False,\n",
        "    text_model_trainable= False\n",
        ")"
      ],
      "metadata": {
        "id": "NDp3eJ0nHm0_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model.compile(Adamax(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")"
      ],
      "metadata": {
        "id": "UNILrVExHm26"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(multimodal_model, show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "5F4mQHJjHm46",
        "outputId": "e32a6c48-0827-4c1f-852b-e132aa6415c2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAEnCAYAAAC0fXf5AAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzde3zMV/4/8NckmWQuMrkQSSRCLtS9VeySsqp29WKpiJCiu+zSuGzTkKoSVF3r8hXfItvVWrtLfxq3pVXaPrCqWrK6KI2VRnaJCE1EkolcZCTv3x++mRoTzCSTmUnyej4e84czZ87n/fmck5m3z+UchYgIiIiIiOihXBwdABEREVFjwKSJiIiIyAJMmoiIiIgswKSJiIiIyAJu9xccP34ca9ascUQsRERERE6hX79+mDlzpkmZ2ZmmK1euYOfOnXYLipzDzp07kZOT4+gwGpUTJ07gxIkTjg6DyC443qk5OXHiBI4fP25WbnamqcaOHTsaNCByLgqFAjNmzMDo0aMdHUqjERMTA4B/K9Q8cLxTc1Iz3u/He5qIiIiILMCkiYiIiMgCTJqIiIiILMCkiYiIiMgCTJqIiIiILGCTpGn//v3w8vLCJ598YovmnE5T3z9b4rEiIqKmyiZJk4jYohmn1dT3z5Z4rIiIqKl64DxN1hg6dCiKi4tt0ZRTcqb9Ky8vx+DBg/HNN984OpRa8VgREVFT1WTuaRIR7NixAxs3bnR0KA1q06ZNyMvLc3QYjQKPFRER2VK9k6Zjx44hJCQECoUC69evBwCsXbsWWq0WLi4u6NWrF/z9/aFUKqHVavHkk09iwIABaNu2LVQqFby9vfHGG2+YtPnVV1+hS5cu8PLygkqlQvfu3fH5558b36+qqsKyZcvw2GOPQa1Wo1WrVggNDcWyZctMZrSuqqrCggULEBISArVajR49eiA1NbXe+5eSkgKtVguNRoO9e/fi+eefh06nQ3BwMLZt22b87LvvvguVSoXWrVtjypQpCAwMhEqlQmRkJNLS0oz14uPj4e7ujoCAAGPZ9OnTodVqoVAocOPGDQBAQkICEhMTkZWVBYVCgYiICADAl19+iZ/97GfQaDTQ6XTo3r079Hq9VftpC43hWH322WfQ6XRYunSpPQ4JERE1JXKf1NRUqaX4oa5cuSIAZN26dcayt956SwBIWlqalJaWyo0bN+S5554TAPLpp59Kfn6+lJaWSnx8vACQM2fOGD+7Y8cOWbhwody8eVMKCgqkb9++0rJlS+P7S5cuFVdXV9m7d6+UlZXJv/71L/H395enn37aJK7XX39dPDw8ZOfOnVJYWChz584VFxcXOXnyZL33LykpSQDIoUOHpLi4WPLy8mTAgAGi1WqlsrLSWC8uLk60Wq2cP39eKioqJD09Xfr06SOenp6SnZ1trDdu3Djx9/c32e6qVasEgOTn5xvLoqOjJTw83PjvW7duiU6nkxUrVkh5eblcv35dRo4cafIZSwCQ1NRUqz5TG2c+ViIi+/btE09PT1m0aFG993XUqFEyatSoerdD1BhwvFNz8qDx3uCX57p06QKNRoOWLVvipZdeAgCEhISgVatW0Gg0GD9+PADgwoULxs+MGjUKb731Fnx8fODr64vhw4ejoKAA+fn5AIA9e/agV69eGD58ONRqNZ588km8+OKLOHr0KCorKwEAFRUVSElJQVRUFKKjo+Ht7Y158+ZBqVRi8+bNNtu/yMhI6HQ6+Pn5ITY2FqWlpcjOzjap4+bmhs6dO8PDwwNdunRBSkoKSkpKbBLHpUuXoNfr0bVrV6hUKvj7+2PXrl1o1apVvdu2NUcfK+DuPVd6vR7z58+3SXtERNR82PWeJnd3dwDAnTt3jGVKpRIAYDAYHvi5mjpVVVUA7iZEct9TWlVVVVAqlXB1dQUAZGRkoKysDN26dTPWUavVCAgIMEnQbKlm/x62LwDQu3dvaDQam8QRFhaG1q1bY/z48Vi4cCEuXbpU7zbtwRHHioiIqD6c8kbwTz/9FE8//TT8/Pzg4eFhds/TCy+8gH/961/Yu3cvysvL8e2332LPnj349a9/bUyaSktLAQDz5s2DQqEwvi5fvoyysjK779P9PDw8jGfO6kOtVuPw4cPo378/li5dirCwMMTGxqK8vNwGUToHWx0rIiKi+nC6pCk7OxtRUVEICAhAWloaiouLsWLFCpM6CxcuxDPPPIMJEyZAp9Nh5MiRGD16NN5//31jHT8/PwBAcnIyRMTkdfz4cbvu0/0MBgOKiooQHBxsk/a6du2KTz75BLm5uZg9ezZSU1OxevVqm7TtaLY+VkRERHVlk3mabOncuXMwGAyYNm0awsLCAAAKhcKkTnp6OrKyspCfnw83t9p3oebpvDNnzjR4zNY6cuQIRAR9+/Y1lrm5uT3yUlVtcnNzUVRUhC5dusDPzw/Lly/HF198gfPnz9syZIex5bEiIiKqD6c70xQSEgIAOHjwICoqKpCZmWnyyDkA/OEPf0BISAhu3br1wHZUKhUmTpyIbdu2ISUlBXq9HlVVVcjJycG1a9cadB/uV11djcLCQty5cwdnz55FQkICQkJCMGHCBGOdiIgI3Lx5E3v27IHBYEB+fj4uX75s1pavry9yc3Nx6dIllJSU4PLly5gyZQouXLiAyspKnD59GpcvXzZJMhqThjxWBoMBBw4c4JQDRERUN/c/TmftlAPr1q2TgIAAASAajUaGDx8ua9euFY1GIwCkffv28tVXX8k777wjXl5eAkD8/f3lww8/lI8++kj8/f0FgPj4+Mi2bdtERGT27Nni6+sr3t7eEhMTI+vXrxcAEh4eLtnZ2XL48GFp2bKlADC+lEqldO7cWXbt2mWM7fbt2zJ79mwJCQkRNzc38fPzk+joaElPT6/X/m3YsMG4fx06dJCsrCzZuHGj6HQ6ASDt2rWTH374QUTuPkavVColKChI3NzcRKfTyYgRIyQrK8tkOwUFBTJo0CBRqVQSGhoqr776qsyaNUsASEREhPGR+1OnTkm7du1ErVZL//79JS0tTSIjI8XHx0dcXV2lTZs2kpSUJHfu3LF4H0VsM+WAsx+r69evy/79+8XT01OWLFlSr30V4SPY1LxwvFNz8qDxrhAxfQxt+/btGDNmjFOvIZaSkoLMzEwkJycbyyorK/Hmm28iJSUFhYWFUKvVDozwJ1OmTMGOHTtQUFDg6FAeSqFQIDU11WRyUHtrLMeqRkxMDABgx44dDo6EqOFxvFNz8qDx7nT3ND3K9evXER8fb3avkru7O0JCQmAwGGAwGJwmaQJ+miqBHo3HioiInJXT3dP0KGq1GkqlEps2bcKPP/4Ig8GA3NxcfPDBB1iwYAFiY2Oh0+ke2saFCxdMpiF40Cs2NtZOe0VERETOrtElTV5eXvjiiy/w/fffo2PHjlCr1ejSpQs2b96Md955B3/9618f2UanTp3MpiGo7fXRRx/VK9a5c+di8+bNKC4uRmhoKHbu3Fmv9pqy5nKspkyZYpKY18yIf6+DBw9izpw52LVrF8LCwox1X375ZbO6Q4YMgaenJ1xdXdG1a1ecOnXKHrtRZ4sWLUKXLl2g0+ng4eGBiIgIvPHGG2YPdVha734VFRXo1KkT5s2b1yTjW7FiBTp16gS1Wg2tVotOnTph/vz5JmtNfvzxx1ixYoXZWds9e/aYjD17rBrA8e7c48nZ43PK8X7/TU51WXuOGj/YaO255qQuN8bGxcWJr6+vHDhwQDIyMqSiosLk/QULFsiwYcNEr9cby8LDw40PPuzbt8+szQMHDsiLL75Yt52ws4EDB8qGDRukoKBA9Hq9pKamilKplOeee65O9e43c+ZMASBJSUlNMr6hQ4fK6tWrJS8vT0pKSmT79u2iVCrlV7/6lUm9tWvXysCBA6WwsNBYVl1dLTk5OXL06FF54YUXTNbztATHu/WcfTw5e3zOON6ZNJGIMGmqi7r+iAQFBdX63vLly6Vjx45SXl5uUh4eHi4ffvihuLi4SFBQkBQVFZm835h+RIYOHWr2ZOfo0aMFgMmizJbWu9fXX38tQ4YMqfeXtDPHFxUVZTY+YmJiBIDk5uaalMfHx0u/fv3EYDCYtfPaa6/ZLWnieHfe8eTs8TnjeG90l+eImqKLFy9i/vz5ePvtt6FSqczej4yMREJCAq5evYrXX3/dARHaxr59+4xLHdWoOW1+7/JGltarUV5ejlmzZmHt2rVNOr7du3ebjY+goCAAMLtUsnDhQpw5c6be22wIHO/OMZ6cPT5nHO9MmoicwLvvvgsRwfDhwx9YZ8mSJejYsSM++OADHDx48KHtiQjWrFmDzp07w8PDAz4+PhgxYoTJwscpKSnQarXQaDTYu3cvnn/+eeh0OgQHB2Pbtm0m7VVVVWHBggUICQmBWq1Gjx49kJqaWr+d/j9Xr16FWq1GaGhoneslJSVh+vTpxuWTbMnZ48vMzIS3tzfatWtnUu7j44OBAwdi7dq1TjeFDMe7844nZ4/P0eOdSRORE/j000/x2GOPQaPRPLCOWq3GX/7yF7i4uGDy5MnGRalrs3DhQsyZMwdJSUnIy8vD0aNHceXKFQwYMAA//vgjAGDatGmYMWMGysvL4enpidTUVGRlZSEsLAyTJ082WarmzTffxMqVK5GcnIxr165h2LBhGDt2LL799tt67XdZWRkOHz6MyZMnw93dvU71vv76a2RlZWHs2LH1iqUxxWcwGHD16lWsX78eBw8exLp162qNr2fPnrh69Sq+++47m23bFjjenWs8OXt8zjTemTQROVhpaSn++9//Ijw8/JF1+/XrhxkzZuDSpUt48803a61TXl6ONWvWYOTIkRg/fjy8vLzQvXt3vPfee7hx4wY2btxo9pnIyEjodDr4+fkhNjYWpaWlyM7OBnD3CZiUlBRERUUhOjoa3t7emDdvHpRKJTZv3lyvfV+2bBkCAwOxZMmSOtUrLy9HQkICUlJS6hVHY4uvbdu2CA4OxsKFC7Fy5UqMGTOm1nodOnQAcHdNT2fB8e5848nZ43Om8f7ApMmSeYz4ajovABgzZozD42hML1tNi5CXlwcReej/uu+1ZMkSPPbYY9iwYQOOHTtm9n56ejpu3bqF3r17m5T36dMH7u7uZms53q/mf3A1//POyMhAWVkZunXrZqyjVqsREBBgcvnDWrt378b27dvx+eefw9PTs0715s6di1deecV4n4MtOXN8V65cQV5eHv7f//t/+Otf/4qePXsiLy/PrF7NmKo52+IMON6dbzw5e3zONN4fOCO4ra7fUuMwZswYJCQkoF+/fo4OpdG4dxmf+qioqAAAeHh4WFRfpVJh8+bN6N+/P373u99hxYoVJu8XFRUBAFq0aGH2WW9vb5SUlFgVX81lkXnz5pnNtxIYGGhVWzU++ugjrFmzBkeOHEGbNm3qVO/YsWM4d+4c1qxZU6cYGnN8SqUSfn5+GDJkCEJDQ9GxY0csW7bM7CbYmpURasaYM+B4d77x5OzxOdN4f2DS5Mg1yMj+xowZg379+rHfrWCrNbhq/tCtWUKmX79+mDlzJlavXo3FixcjJCTE+J63tzcA1PpjUVRUhODgYKviq7mZMzk5GQkJCVZ9tjbr1q3D559/jsOHD9f6Q2dpvU2bNuHQoUNwcTE/Yb506VIsXboUJ0+eNDsD0djju19ERARcXV2Rnp5u9l5lZSUAONWyUhzvdavH8X6Xo8c772kicrDWrVtDoVCguLjYqs8tXrwYnTp1wunTp03Ku3XrhhYtWpjdtJqWlobKykr06tXLqu20bdsWKpXKbL1Ha4kIZs+ejXPnzmHPnj0P/IK2tN7mzZvNZvHPz88HcPfpHRGx6gva2eMrKCio9ebazMxMVFVVoW3btmbv1Ywpf39/i7fT0Dje61aP4/0uR493Jk1EDqbRaBAWFoacnByrPldz2eL++VNUKhUSExOxe/dubN26FXq9HufOncPUqVMRGBiIuLg4q7czceJEbNu2DSkpKdDr9aiqqkJOTg6uXbsGAIiNjYW/v/9Dl7U4f/48Vq5ciffffx9KpdLsHrHVq1dbVc8aTSE+rVaLL774AocPH4Zer4fBYMDp06fx29/+FlqtFjNnzjT7TM2Y6t69u9UxNRSOd+cYT84en7OOdyZNRE5g6NChSE9PR3l5ubHs73//OyIiIpCVlYU+ffrg1VdfNftc3759a/3yeOutt7Bs2TIsWrQIrVq1wsCBA9G+fXscOXIEWq0WwN15a2ruy+rRowf+85//4P3330diYiIA4LnnnkNmZiYAYO3atZgxYwZWrFiBli1bIjAwEAkJCSgsLARw97R4Xl4e9u7d+8B9tHTulIaYY6UpxKdSqfDUU09h0qRJCAoKgqenJ2JiYtC+fXucOHHC5MblGidPnkRQUBB69Ohh85jrg+Pd+nrWaArxOe14v3+KcC6j0jyBy6hYzZbLSmRmZoqbm5ts2bLFVuHZVVVVlQwYMEA2bdrk6FBq1Rzju3HjhqhUKlm9erXZe45eRoXjvWE1x/jsNd55ponIzsrLy/H5558jMzPTeONiREQEFi1ahEWLFj1y5XBnU1VVhT179qCkpASxsbGODsdMc41v4cKFeOKJJxAfHw/g7hmD3NxcHDt2DBcvXrTZdh6F492+mmt89hrvdkmaTpw4gc6dO8PFxQUKhQL+/v6PnDzL3nbt2oWwsDDjddqAgACMHz/e0WFRE3Tz5k0899xz6NixI373u98Zy+fMmYOYmBjExsZafZOsIx05cgS7du3CgQMHLJ57x56aY3xr1qzBmTNnsH//fiiVSgDA3r17ERQUhAEDBuDTTz+1yXYswfFuX80xPnuOd4WI6QXL7du3Y8yYMQ1yHfO5557D559/jsLCQuNjos4mIiICN27cMM790VwoFAqkpqZyygErxMTEALDd1AM1am5+fOedd2zaLjUPe/fuxfnz5/HGG2+Y3TRdHxzv5IzsPd6b7eW58vJyREZGOjoM+j/26I/G0udDhgzhDwjV2Ysvvog5c+bY9AekIXG8U33Ye7w326Rp06ZNtU7DTo5hj/5gnxMRUX04NGlKSUmBVquFRqPB3r178fzzz0On0yE4OBjbtm0z1nv33XehUqnQunVrTJkyBYGBgVCpVIiMjDRZVyg+Ph7u7u4ICAgwlk2fPh1arRYKhQI3btwAACQkJCAxMRFZWVlQKBSIiIioU/xfffUVunTpAi8vL6hUKnTv3h2ff/45AGDSpEnG+6PCw8ONE7JNnDgRGo0GXl5e+PjjjwHcvTFuwYIFCAkJgVqtRo8ePYzL2KxcuRIajQaenp7Iy8tDYmIigoKCkJGRUaeYbUVEsGbNGnTu3BkeHh7w8fHBiBEjTNZmqk9/2KvPP/vsM+h0OixdurRBjxcRETUB9z9O15BTDjz77LMCQAoLC41lSUlJAkAOHTokxcXFkpeXJwMGDBCtViuVlZXGenFxcaLVauX8+fNSUVEh6enp0qdPH/H09JTs7GxjvXHjxom/v7/JdletWiUAJD8/31gWHR0t4eHhZjGGh4eLl5eXRfuzY8cOWbhwody8eVMKCgqkb9++Jo81RkdHi6urq1y9etXkc2PHjpWPP/7Y+O/XX39dPDw8ZOfOnVJYWChz584VFxcXOXnypMkxeu2112TdunUycuRI+fe//21RjJaClVMOLFiwQNzd3WXLli1SVFQkZ8+elSeffFJatWol169fN9arT3/Yo8/37dsnnp6esmjRIov3vUZdHsEmaqw43qk5cfopByIjI6HT6eDn54fY2FiUlpYiOzvbpI6bm5vxzEaXLl2QkpKCkpISbN682SExjxo1Cm+99RZ8fHzg6+uL4cOHo6CgwDh1/NSpU1FVVWUSn16vx8mTJ/HCCy8AuLuwYEpKCqKiohAdHQ1vb2/MmzcPSqXSbL/eeecd/OEPf8CuXbvQqVMn++3ofcrLy7FmzRqMHDkS48ePh5eXF7p374733nsPN27cwMaNG222rYbu86FDh0Kv12P+/Pk2aY+IiJoup0ma7uXu7g4AMBgMD63Xu3dvaDQak0tCjlTzqGPNQpTPPPMMOnbsiD//+c/GpxE/+ugjxMbGGm9ay8jIQFlZmcnspmq1GgEBAU6zX/dLT0/HrVu3zNYR6tOnD9zd3U0un9mas/U5ERE1H06ZNFnDw8PDeGbH3j799FM8/fTT8PPzg4eHB9544w2T9xUKBaZMmYL//Oc/OHToEADgb3/7G37/+98b65SWlgIA5s2bZ7Kez+XLl1FWVma/nbFCzXQMtS3c6O3tXetq47bkyD4nIqLmq1EnTQaDAUVFRQgODrbL9o4ePWpcuyg7OxtRUVEICAhAWloaiouLsWLFCrPPTJgwASqVCh988AEyMjKg0+nQrl074/t+fn4AgOTkZLMVoo8fP26X/bJWzRxbtSVHDd0f9u5zIiKiGm6ODqA+jhw5AhFB3759jWVubm6PvKxXV//617+Miz+eO3cOBoMB06ZNQ1hYGIC7Z5bu5+PjgzFjxuCjjz6Cp6cnJk+ebPJ+27ZtoVKpcObMmQaJuSF069YNLVq0wLfffmtSnpaWhsrKSvTq1ctYZuv+sHefExER1WhUZ5qqq6tRWFiIO3fu4OzZs0hISEBISAgmTJhgrBMREYGbN29iz549MBgMyM/Px+XLl83a8vX1RW5uLi5duoSSkpKH/ugaDAb8+OOPJitmh4SEAAAOHjyIiooKZGZmPvBenqlTp+L27dvYt28fhg0bZvKeSqXCxIkTsW3bNqSkpECv16Oqqgo5OTm4du2atYfILlQqFRITE7F7925s3boVer0e586dw9SpUxEYGIi4uDhj3fr2R0P3+YEDBzjlABERWeb+x+kaYsqBEydOSNeuXcXFxUUASEBAgCxdulQ2bNggGo1GAEiHDh0kKytLNm7cKDqdTgBIu3bt5IcffhCRu4+fK5VKCQoKEjc3N9HpdDJixAjJysoy2VZBQYEMGjRIVCqVhIaGyquvviqzZs0SABIREWF8VP3UqVPSrl07UavV0r9/f/njH/8o4eHhAuChr927dxu3NXv2bPH19RVvb2+JiYmR9evXCwAJDw83eSReRKRnz54yZ86cWo/P7du3Zfbs2RISEiJubm7i5+cn0dHRkp6eLitWrBC1Wi0ApG3btg22KjisnHKgurpaVq1aJR06dBClUik+Pj4SFRUlGRkZJvXq2h/Xr19v8D6/fv267N+/Xzw9PWXJkiVWHzM+gk3NCcc7NScPGu92XXuuPqZMmYIdO3agoKDA0aHUydChQ7F+/XqEhoY6OpRaOePac87e5w21FheRM+J4p+akSaw9V/Mof2Nw7+W+s2fPQqVSOW3C5MwaU58TEVHT1qhvBHdms2fPxtSpUyEimDhxIrZs2eLokIiIiKgeGsWZprlz52Lz5s0oLi5GaGgodu7c6eiQHkmj0aBTp0745S9/iYULF6JLly6ODqlRaYx9TkRETVujSJqWLVuG27dvQ0Tw3//+F6NGjXJ0SI+0ZMkSVFVVITs72+yJOXq0xtjnRETUtDWKpImIiIjI0Zg0EREREVmASRMRERGRBZg0EREREVnggVMObN++3Z5xkBNw1gWCnVVOTg4A/q1Q88DxTs1JTk5OrQvDP3BGcCIiIqLmatSoUWYzgpslTUREjuaMy/oQEfGeJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILKEREHB0EETVfcXFxyMjIMCk7deoUQkND4ePjYyxzdXXFX//6VwQHB9s7RCIiAICbowMgoubN398fGzduNCs/e/asyb/DwsKYMBGRQ/HyHBE51NixYx9Zx93dHRMmTGj4YIiIHoKX54jI4bp164bz58/jYV9HGRkZ6Nixox2jIiIyxTNNRORwv/nNb+Dq6lrrewqFAo8//jgTJiJyOCZNRORwL730Eqqqqmp9z9XVFb/97W/tHBERkTleniMipxAZGYm0tDRUV1eblCsUCly5cgVBQUEOioyI6C6eaSIip/Dyyy9DoVCYlLm4uKB///5MmIjIKTBpIiKnEBMTY1amUCjwm9/8xgHREBGZY9JERE6hVatWGDx4sMkN4QqFAlFRUQ6MiojoJ0yaiMhpjB8/3jjtgKurK5599lm0bNnSwVEREd3FpImInMbIkSPh7u4OABARjB8/3sERERH9hEkTETkNrVaLX//61wDuzgI+bNgwB0dERPQTJk1E5FTGjRsHAIiKioJWq3VwNEREP3H6eZrufwSZiIiImqbU1FSMHj3a0WE8kJujA7BEQkIC+vXr5+gwyIkdP34ca9euRWpqqqNDaVTGjBnjlH9fW7duRWxsLNzcGsVXFDmIs45fqpsxY8Y4OoRHahRnmpw98yTH2759O8aMGfPQBV/JnLP+fVVUVEClUjk6DHJyzjp+qW4aQ3/yniYicjpMmIjIGTFpIiIiIrIAkyYiIiIiCzBpIiIiIrIAkyYiIiIiCzTbpGn//v3w8vLCJ5980iD1m7NJkybB09MTCoUCZ86ccXQ4VmE/ExHRgzTbpMnaR9P5KLvlPvjgA7z//vuODqNO2M9ERPQgzXbmuKFDh6K4uLjB6lPj5Ez9XF5ejsGDB+Obb75xdChERIRmfKaJGhaXv6m/TZs2IS8vz9FhEBHR/2lSSVPnzp2hUCjg4uKCXr16oaysDADwxhtvwMvLCyqVCn/5y19w7NgxhISEQKFQYP369cbPf/nll/jZz34GjUYDnU6H7t27Q6/XP7C+iGDNmjXo3LkzPDw84OPjgxEjRuDChQvGOikpKdBqtdBoNNi7dy+ef/556A9pv4YAACAASURBVHQ6BAcHY9u2bXXaz6qqKixYsAAhISFQq9Xo0aOHcfkQa7e3ZcsW9O7dGyqVClqtFu3bt8fixYst3r+aeqtWrcJjjz0GDw8PeHl5YdasWVbFvXLlSmg0Gnh6eiIvLw+JiYkICgpCRkZGnY5RXdTWz5Yez3fffRcqlQqtW7fGlClTEBgYCJVKhcjISKSlpRnrxcfHw93dHQEBAcay6dOnQ6vVQqFQ4MaNGwDuLh2UmJiIrKwsKBQKREREAAA+++wz6HQ6LF261B6HhIiI7iVODoCkpqZaVPfOnTvSvn17CQkJkTt37pi8N2PGDElOTjb++8qVKwJA1q1bJyIit27dEp1OJytWrJDy8nK5fv26jBw5UvLz82utLyKyYMECcXd3ly1btkhRUZGcPXtWnnzySWnVqpVcv37dWC8pKUkAyKFDh6S4uFjy8vJkwIABotVqpbKy0upj8vrrr4uHh4fs3LlTCgsLZe7cueLi4iInT560anvJyckCQJYvXy4FBQVy8+ZN+dOf/iTjxo2zev8UCoX8z//8jxQWFkpZWZls2LBBAMjp06etjvu1116TdevWyciRI+Xf//63RcckNTVVbDGca+tnS49nXFycaLVaOX/+vFRUVEh6err06dNHPD09JTs721hv3Lhx4u/vb7LdVatWCQDjeBMRiY6OlvDwcJN6+/btE09PT1m0aFG991XEur8vImfD8du0NIb+bFJJk8hPicD27duNZaWlpRISEiLFxcXGsvt/HL///nsBIPv27au13fvrl5WVSYsWLSQ2Ntak3j//+U8BYPKjVvOjW15ebiyrSSouXrxo8b6JiJSXl4tGozHZbllZmXh4eMi0adMs3l5lZaV4e3vLoEGDTNq/c+eOrF271uL9KysrE41GI7/61a9M6m3bts0kaapr3JayR9L0qP6Li4sTLy8vk/ZOnjwpAOTtt982ltUnabK1xvAlRfQgHL9NS2PozyZ1eQ64+7i7l5cX1q5dayzbunUrRowYAZ1O98DPhYWFoXXr1hg/fjwWLlyIS5cuPXQ76enpuHXrFnr37m1S3qdPH7i7u5tckqmNu7s7AMBgMDxij0xlZGSgrKwM3bp1M5ap1WoEBASYXTZ72PbOnj2LoqIiPPvssyb1XF1d8dprr1m8fxcvXkRZWRkGDx7cIHE7K0v7r3fv3tBoNI1yH4mIyFSTS5patGiBV155Bd988w3++c9/AgD++Mc/Ij4+/qGfU6vVOHz4MPr374+lS5ciLCwMsbGxKC8vr7V+UVGRcXv38/b2RklJST33pHalpaUAgHnz5kGhUBhfly9fNt7DZQm9Xm+MtTaW7l9OTg4AwM/Pzy5xN0YeHh7Iz893dBhERFRPTS5pAu7ebKtUKpGcnIyjR4+ibdu2CA8Pf+Tnunbtik8++QS5ubmYPXs2UlNTsXr16lrr1iQbtSVHRUVFCA4Ort9OPEBNcpKcnAy5e3nV+Dp+/LjF7bRp0wYAjDce38/S/atZjf727dt2ibuxMRgMDToeiIjIfppk0hQcHIzRo0dj586dmD9/PhISEh75mdzcXJw/fx7A3R/45cuX48knnzSW3a9bt25o0aIFvv32W5PytLQ0VFZWolevXvXfkVq0bdsWKpWq3jNtt2/fHr6+vvjiiy9qfd/S/evWrRtcXFzw5Zdf2iXuxubIkSMQEfTt29dY5ubmZvVlWSIicrwmmTQBQGJiIu7cuYPCwkI888wzj6yfm5uLKVOm4MKFC6isrMTp06dx+fJlkx+7e6lUKiQmJmL37t3YunUr9Ho9zp07h6lTpyIwMBBxcXG23iXjdidOnIht27YhJSUFer0eVVVVyMnJwbVr1yxux8PDA3PnzsXRo0cRHx+Pq1evorq6GiUlJTh//rzF++fn54fo6Gjs3LkTmzZtgl6vx9mzZ7Fx48YGidvZVVdXo7CwEHfu3MHZs2eRkJCAkJAQTJgwwVgnIiICN2/exJ49e2AwGJCfn4/Lly+bteXr64vc3FxcunQJJSUlMBgMOHDgAKccICJyFIfdgm4h1ONu+kGDBskHH3xgVr5u3ToJCAgQAKLRaGT48OFy6dIliYyMFB8fH3F1dZU2bdpIUlKS3Llzp9b6IiLV1dWyatUq6dChgyiVSvHx8ZGoqCjJyMgwbmvDhg2i0WgEgHTo0EGysrJk48aNotPpBIC0a9dOfvjhB6v26/bt2zJ79mwJCQkRNzc38fPzk+joaElPT7d6e+vXr5fu3buLSqUSlUolPXv2lA0bNli8fyIiJSUlMmnSJGnZsqW0aNFC+vfvLwsWLBAAEhwcLN99990j416xYoWo1WoBIG3btpUtW7ZYdUxs8fRcbf1szfGMi4sTpVIpQUFB4ubmJjqdTkaMGCFZWVkm2ykoKJBBgwaJSqWS0NBQefXVV2XWrFkCQCIiIozTE5w6dUratWsnarVa+vfvL9evX5f9+/eLp6enLFmypF77WqM+f19Ejsbx27Q0hv5UiDj3YlsKhQKpqakYPXq0o0MhJ7Z9+3aMGTPGoWvHTZkyBTt27EBBQYHDYrAW/76oMeP4bVoaQ3822ctzRI5QVVXl6BCIiKiBMGlyAhcuXDB5DP9Br9jYWEeHSmR08OBBzJkzB7t27UJYWJhxnL788stmdYcMGQJPT0+4urqia9euOHXqlAMittyiRYvQpUsX6HQ6eHh4ICIiAm+88QZu3bpVp3r3q6ioQKdOnTBv3rwmGd+KFSvQqVMnqNVqaLVadOrUCfPnzzdOdQIAH3/8MVasWOGw/2g05fG7ZMmSWn9D7p0n717V1dVITk5GZGRkre9bMo4c3Z924+DLg4+ERnCNkxzPVjOC19WcOXPE3d1dAEj79u1lx44dDovFGnX9+1qwYIEMGzZM9Hq9sSw8PFxatmz5wJn1Dxw4IC+++GK94rWXgQMHyoYNG6SgoED0er2kpqaKUqmU5557rk717jdz5kwBIElJSU0yvqFDh8rq1aslLy9PSkpKZPv27aJUKs1WDli7dq0MHDhQCgsL67Qdjt/aLV68WACYvbp27WpW94cffpCnnnpKAMjjjz9ea3uWjiNH9ac9MWmiJsHRSVNjVZe/r+XLl0vHjh3NlrsJDw+XDz/8UFxcXCQoKEiKiopM3m9MPzpDhw41W79y9OjRAsBkHUFL693r66+/liFDhtQ7KXHm+KKioszGR0xMjACQ3Nxck/L4+Hjp16+fGAwGq7fD8Vu7xYsXW/QwzZkzZ2TkyJGydetWeeKJJx6YNFkzjuzdn/bGy3NEZLGLFy9i/vz5ePvtt40Tm94rMjISCQkJuHr1Kl5//XUHRGgb+/btg6urq0lZq1atAMBkBntL69UoLy/HrFmzTJZ5aorx7d6922x8BAUFAYDZpcGFCxfizJkz9d6mJZrL+LXU448/jl27dmHcuHHw8PB4YD1rxpE9+9MRmDQRkcXeffddiAiGDx/+wDpLlixBx44d8cEHH+DgwYMPbU9EsGbNGnTu3BkeHh7w8fHBiBEjTNbqS0lJgVarhUajwd69e/H8889Dp9MhODgY27ZtM2mvqqoKCxYsQEhICNRqNXr06IHU1NT67fT/uXr1KtRqNUJDQ+tcLykpCdOnT3/kskNNMb7MzEx4e3ujXbt2JuU+Pj4YOHAg1q5d2+BPvzbn8WtrDxpH9uxPR2DSREQW+/TTT/HYY49Bo9E8sI5arcZf/vIXuLi4YPLkycZ1B2uzcOFCzJkzB0lJScjLy8PRo0dx5coVDBgwAD/++CMAYNq0aZgxYwbKy8vh6emJ1NRUZGVlISwsDJMnTzaZXf3NN9/EypUrkZycjGvXrmHYsGEYO3as2cz21iorK8Phw4cxefJk42LN1tb7+uuvkZWVhbFjx9YrlsYUn8FgwNWrV7F+/XocPHgQ69atqzW+nj174urVq/juu+9stu3aNKfxO2fOHPj4+MDd3R2hoaEYMWIETp48aXU7tXnUeLNXfzqEI68NWgKN4BonOR7vaaoba/6+bt26JQqFQoYNG1br++Hh4fLf//7X+O/ExEQBIH/4wx9ExPyekLKyMmnRooXExsaatPPPf/5TAMiiRYuMZUlJSQLA5D6UDRs2CAC5ePGiiIiUl5eLRqMxaa+srEw8PDxk2rRpFu3jgyQlJUnHjh1Nbhy2pl5ZWZn07t1bcnJyREQkPz+/XvcMNZb4/P39BYC0bNlS/vd//1cqKytrrffnP/9ZAMjf/vY3q9rn+K1ddna2nDp1SkpKSuT27dty/Phx6dmzp6jVavn+++9r/czPf/7zB97TdL9HjTd79KejuNk7SauLprygK9lGzRjZvn27gyNpuvLy8iAiD/1f+r2WLFmCffv2YcOGDRgzZozZ++np6bh16xZ69+5tUt6nTx+4u7sjLS3toe3X/A+35n/qGRkZKCsrM3msWq1WIyAgwORyibV2796N7du344svvoCnp2ed6s2dOxevvPKK8b4eW3Lm+K5cuYKioiKcPn0ac+bMwcaNG3H48GG0bt3apF7NmKo5O9MQmtP4bdu2Ldq2bWv8d9++fbF582Y88cQT2LBhA1JSUqxq716WjDd79KejNIqkae3atU32pjKyrdq+3Mg2KioqAOChN4zeS6VSYfPmzejfvz9+97vfYcWKFSbvFxUVAQBatGhh9llvb2+UlJRYFV/NZZR58+aZzS8UGBhoVVs1PvroI6xZswZHjhxBmzZt6lTv2LFjOHfuHNasWVOnGBpzfEqlEn5+fhgyZAhCQ0PRsWNHLFu2zOz7XK1WA/hpjDWE5jh+79W9e3e4urrihx9+qHMblo43e/SnozSKe5pSU1Mhd6dH4IuvWl81N0s6Oo7G9rJGzRehNZPX9evXDzNnzkRmZiYWL15s8p63tzcA1PrjUlRUhODgYKviq7l5OTk52Ww/63K2et26ddi6dSsOHz780B+IR9XbtGkTDh06BBcXF+MkgzWxLl26FAqFok73rDh7fPeLiIiAq6sr0tPTzd6rrKwE8NMYawjNbfzer7q6GtXV1RYnjfezdLwB9ulPR2kUSRMROV7r1q2hUChQXFxs1ecWL16MTp064fTp0ybl3bp1Q4sWLcx+kNPS0lBZWYlevXpZtZ22bdtCpVLhzJkzVn3ufiKC2bNn49y5c9izZ0+tZxKsqbd582azH8H8/HwAd59WExGzSzyNOb6CgoJabybPzMxEVVWVyWWjGjVjyt/f3+LtWKu5jF8AePbZZ83KTp48CRFBv379rGrL0nF0L3v0p6MwaSIii2g0GoSFhSEnJ8eqz9Vc5rh/nheVSoXExETs3r0bW7duhV6vx7lz5zB16lQEBgYiLi7O6u1MnDgR27ZtQ0pKCvR6PaqqqpCTk4Nr164BAGJjY+Hv7//QZTDOnz+PlStX4v3334dSqTRbimL16tVW1bNGU4hPq9Xiiy++wOHDh6HX62EwGHD69Gn89re/hVarxcyZM80+UzOmunfvbnVMlmou4xe4Ox3ARx99hKKiIhgMBhw/fhyTJk1CSEgIpk6dalVcdRlH9uhPR2HSREQWGzp0KNLT01FeXm4s+/vf/46IiAhkZWWhT58+ePXVV80+17dv31p/LN966y0sW7YMixYtQqtWrTBw4EC0b98eR44cgVarBXB3npvk5GQAQI8ePfCf//wH77//PhITEwEAzz33HDIzMwHcvf9xxowZWLFiBVq2bInAwEAkJCSgsLAQwN3LBnl5edi7d+8D99HSy5bWXt60RFOIT6VS4amnnsKkSZMQFBQET09PxMTEoH379jhx4kSt65+dPHkSQUFB6NGjh81jvldzGL81bc6bNw/BwcHQaDQYPXo0nnrqKZw4cQItW7Y01jtx4gT69++PNm3aIC0tDd999x0CAwPx1FNP4ejRowDqNo7s1Z8OIU4OjeARRHI8TjlQN9b+fWVmZoqbm5tFSzQ4o6qqKhkwYIBs2rTJ0aHUqjnGd+PGDVGpVLJ69WqrP8vx63zs2Z+OwDNNRGSxiIgILFq0CIsWLTJbDsPZVVVVYc+ePSgpKUFsbKyjwzHTXONbuHAhnnjiCcTHx9uszQfh+G149uxPR2DSRERWmTNnDmJiYhAbG2v1TbWOdOTIEezatQsHDhyweK4ee2qO8a1ZswZnzpzB/v37oVQqbdLmo3D8NhxH9Ke9MWmqxa5duxAWFmZ2w9u9r/bt2zs6TBP79++Hl5cXPvnkE4dsf/Xq1canU9577z2HxED2s3TpUsTHx2P58uWODsVigwcPxocffoiAgABHh1Kr5hbf3r17cfv2bRw5cgQ+Pj42adNSHL+258j+tKdGMbmlvUVHRyM6OhoRERG4ceOGcRKzqqoqVFZWoqSkBE8//bRjg7yPNMBNn9Z4/fXXMWLECHTo0MGhcZD9DBkyBEOGDHF0GNRIvfjii3jxxRcdtn2OX9tydH/aC880WcHV1RVqtRqtW7dGx44dHRZHeXk5IiMjTcqGDh2K4uJiDBs2zEFRUW390hi3QUREtWPSVEd79uxx2LY3bdqEvLw8h22famePfmHfExE5DpOmeoqPj4e7u7vJdebp06dDq9VCoVDgxo0bAO7O1aHVaqHRaLB37148//zz0Ol0CA4OxrZt28za3bJlC3r37g2VSgWtVov27dtj8eLFSEhIQGJiIrKysqBQKBAREYFjx44hJCQECoUC69evN7YhIlizZg06d+4MDw8P+Pj4YMSIESaLP1oT11dffYUuXbrAy8sLKpUK3bt3x+eff27rQ2o3lhwfS/u3tn559913oVKp0Lp1a0yZMgWBgYFQqVSIjIw0WcyzPtsAgM8++ww6nQ5Lly5t0ONFRNTsOXjKg0eCA+dtCA8PFy8vL5OyQ4cOyapVq0zKxo0bJ/7+/iZlq1atEgCSn59vLEtKShIAcujQISkuLpa8vDwZMGCAaLVaqaysNNZLTk4WALJ8+XIpKCiQmzdvyp/+9CcZN26ciIhER0dLeHi4yfauXLkiAGTdunXGsgULFoi7u7ts2bJFioqK5OzZs/Lkk09Kq1at5Pr161bHtWPHDlm4cKHcvHlTCgoKpG/fvtKyZUvj+5mZmQJA/vjHP1p8jG2lLvM0WXp8LO3f2volLi5OtFqtnD9/XioqKiQ9PV369Okjnp6ekp2dbZNt7Nu3Tzw9PWXRokVW7b9I45gXhehBOH6blsbQnzzT9AjFxcUmT80NHjy43m1GRkZCp9PBz88PsbGxKC0tRXZ2NgDAYDDg7bffxqBBg/Dmm2/C19cXPj4++P3vf48+ffpYvI3y8nKsWbMGI0eOxPjx4+Hl5YXu3bvjvffew40bN7Bx40ar4gKAUaNG4a233oKPjw98fX0xfPhwFBQUGNepakzqcnzqys3NzXg2q0uXLkhJSUFJSQk2b95sk/aHDh0KvV6P+fPn26Q9IiKqHZOmR/Dy8jJZyPIf//iHTdt3d3cHcDdZAoCzZ8+iqKjIbMFFV1dXvPbaaxa3m56ejlu3bpkttNmnTx+4u7ubXB6yJK7a1MzDYc2q4c6ivsenPnr37g2NRmNyGZCIiJwfpxyw0tNPP92g0w3o9XoAgLe3d73aqZkmobYVqb29vVFSUmJ1m59++ilWrVqF9PR040KcjVVDHB9reHh4NMozdEREzRnPNDmZNm3aAIDx5t+6qkm6avvxLyoqQnBwsFXtZWdnIyoqCgEBAUhLS0NxcTFWrFhRrxgdydbHxxoGg6HBt0FERLbHpMkG3NzcbHbWpX379vD19cUXX3xRr3a6deuGFi1a4NtvvzUpT0tLQ2VlJXr16mVVe+fOnYPBYMC0adMQFhYGlUoFhUJRrxgdyZrjY8v+Be4uhyAi6Nu3b4Ntg4iIbI9Jkw1ERETg5s2b2LNnDwwGA/Lz83H58uU6teXh4YG5c+fi6NGjiI+Px9WrV1FdXY2SkhKcP38eAODr64vc3FxcunQJJSUltf7YqlQqJCYmYvfu3di6dSv0ej3OnTuHqVOnIjAwEHFxcVbFFRISAgA4ePAgKioqkJmZ2aD3/TQ0a46Ppf37oH6prq5GYWEh7ty5g7NnzyIhIQEhISGYMGGCTbZx4MABTjlARGQPjn1479HggEcQv/76a+nYsaMAEAASEBAggwcPfmD9goICGTRokKhUKgkNDZVXX31VZs2aJQAkIiJCsrOzZcOGDaLRaASAdOjQQbKysmTjxo2i0+kEgLRr105++OEHY5vr16+X7t27i0qlEpVKJT179pQNGzaIiMipU6ekXbt2olarpX///jJv3jwJCAgQAKLRaGT48OEiIlJdXS2rVq2SDh06iFKpFB8fH4mKipKMjAzjdqyJa/bs2eLr6yve3t4SExMj69evFwASHh4uCQkJ4u/vLwBEq9XKyJEjG6JrHqguUw5YcnxELOtfEfN+uX79usTFxYlSqZSgoCBxc3MTnU4nI0aMkKysLJttY//+/eLp6SlLliyx+rg54u+LyFY4fpuWxtCfChEHL1r2CAqFAqmpqRg9erSjQyEntn37dowZM8bha/Ddb8qUKdixYwcKCgocHUqt+PdFjRnHb9PSGPqTl+eIGlhjnJKBiIjMMWkiIiIisgCTJqIGMnfuXGzevBnFxcUIDQ3Fzp07HR0SERHVAye3JGogy5Ytw7JlyxwdBhER2QjPNBERERFZgEkTERERkQWYNBERERFZgEkTERERkQUaxY3gycnJ2LFjh6PDICeWk5MDAIiJiXFwJI0P/76oMeP4JXty+hnB+SNI1PwcOHAAPXv2REBAgKNDISI7mjlzJvr16+foMB7I6ZMmImp+GsNyCkTU/PCeJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisgCTJiIiIiILMGkiIiIisoCbowMgouatqKgIImJWXlpaisLCQpOyFi1aQKlU2is0IiITCqnt24qIyE6eeeYZ/OMf/3hkPVdXV1y9ehX+/v52iIqIyBwvzxGRQ7300ktQKBQPrePi4oJf/OIXTJiIyKGYNBGRQ40aNQpubg+/U0ChUOA3v/mNnSIiIqodkyYicigfHx8MGTIErq6uD6zj4uKCqKgoO0ZFRGSOSRMROdz48eNRXV1d63tubm4YOnQovLy87BwVEZEpJk1E5HDDhw+Hh4dHre9VVVVh/Pjxdo6IiMgckyYicjiNRoOoqKhapxNQq9V44YUXHBAVEZEpJk1E5BTGjh0Lg8FgUqZUKjFq1Cio1WoHRUVE9BMmTUTkFJ599lmz+5YMBgPGjh3roIiIiEwxaSIip6BUKhEbGwt3d3djmbe3NwYPHuzAqIiIfsKkiYicxksvvYTKykoAd5Oo8ePHP3IOJyIie+EyKkTkNKqrq9GmTRv8+OOPAIBjx47hqaeecnBURER38UwTETkNFxcXvPzyywCAwMBAREZGOjgiIqKf8Ly3g+Xk5OCbb75xdBhETqNVq1YAgJ///OfYsWOHg6Mhch5t27ZFv379HB1Gs8bLcw62fft2jBkzxtFhEBGRkxs1ahT/I+FgPNPkJJi72l9MTAwA8EvICjVJfkOP1507d2LUqFENug2yX39S/dV8X5Fj8Z4mInI6TJiIyBkxaSIiIiKyAJMmIiIiIgswaSIiIiKyAJMmIiIiIgswaSIiIiKyAJOmJmDSpEnw9PSEQqHAmTNnHB1OvVRXVyM5OdnimaArKirQqVMnzJs3r4Eje7D9+/fDy8sLn3zyicNiICKihsekqQn44IMP8P777zs6jHrLzMzEL37xC8ycORNlZWUWfSYpKQkZGRkNHNnDcY4bIqLmgZNbklP47rvvsGjRIkydOhWlpaUWJSLffPMNvv/+eztE93BDhw5FcXGxo8MAAJSXl2Pw4MFcmoeIqAHwTFMToVAoHB1CvTz++OPYtWsXxo0bBw8Pj0fWLy8vx6xZs7B27Vo7RNd4bNq0CXl5eY4Og4ioSWLS1AiJCFatWoXHHnsMHh4e8PLywqxZs8zqVVVVYcGCBQgJCYFarUaPHj2QmpoKAEhJSYFWq4VGo8HevXvx/PPPQ6fTITg4GNu2bTNp58svv8TPfvYzaDQa6HQ6dO/eHXq9/pHbaEhJSUmYPn06/Pz8GnxbD3Ps2DGEhIRAoVBg/fr1ACw/tu+++y5UKhVat26NKVOmIDAwECqVCpGRkUhLSzPWi4+Ph7u7OwICAoxl06dPh1arhUKhwI0bNwAACQkJSExMRFZWFhQKBSIiIgAAn332GXQ6HZYuXWqPQ0JE1GQxaWqE5s+fj9mzZyMuLg4//vgjrl+/jjfffNOs3ptvvomVK1ciOTkZ165dw7BhwzB27Fh8++23mDZtGmbMmIHy8nJ4enoiNTUVWVlZCAsLw+TJk2EwGAAApaWlGD58OEaNGoWbN28iMzMTHTt2RGVl5SO30VC+/vprZGVlYezYsQ22DUv179/f7FKYpcc2Pj4eEyZMQFlZGV577TVcunQJp06dwp07d/CrX/0KV65cAXA3uRo9erTJNjZs2IC3337bpGzt2rUYNmwYwsPDISK4ePEigLuJLXD3JnsiIqo7Jk2NTHl5OZKTk/HLX/4SM2fOhLe3N9RqNXx9fU3qVVRUICUlBVFRUYiOjoa3tzfmzZsHpVKJzZs3m9SNjIyETqeDn58fYmNjUVpaiuzsbADApUuXoNfr0bVrV6hUKvj7+2PXrl1o1aqVVduw5f4nJCX+DAAAFXdJREFUJCQgJSWlQdq3tYcd2xpubm7o3LkzPDw80KVLF6SkpKCkpMRmx3Do0KHQ6/WYP3++TdojImqumDQ1MhcvXkRZWRkGDx780HoZGRkoKytDt27djGVqtRoBAQG4cOHCAz/n7u4OAMazIWFhYWjdujXGjx+PhQsX4tKlS/XeRn3MnTsXr7zyCoKCghqk/YZ0/7F9kN69e0Oj0TTYMSQiorph0tTI5OTkAMAj7+UpLS0FAMybNw8KhcL4unz5ssWP8wN3k6DDhw+jf//+WLp0KcLCwhAbG4vy8nKbbcNSx44dw7lz5zBp0iSbt+1sPDw8kJ+f7+gwiIjoHkyaGhmVSgUAuH379kPr1SRVycnJEBGT1/Hjx63aZteuXfHJJ58gNzcXs2fPRmpqKlavXm3TbVhi06ZNOHToEFxcXIwJWk0MS5cuhUKhaNB7qezFYDCgqKgIwcHBjg6FiIjuwaSpkenWrRtcXFzw5ZdfPrRe27ZtoVKp6j1DeG5uLs6fPw/gbiK2fPlyPPnkkzh//rzNtmGpzZs3myVnNWdjkpKSICLo3bu3XWJpSEeOHIGIoG/fvsYyNze3R17WIyKihsWkqZHx8/NDdHQ0du7ciU2bNkGv1+Ps2bPYuHGjST2VSoWJEydi27ZtSElJgV6vR1VVFXJycnDt2jWLt5ebm4spU6bgwoULqKysxOnTp3H58mX07dvXZtto7qqrq1FYWIg7d+7g7NmzSEhIQEhICCZMmGCsExERgZs3b2LPnj0wGAzIz8/H5cuXzdry9fVFbm4uLl26hJKSEhgMBhw4cIBTDhAR2YKQQ6Wmpoq13VBSUiKTJk2Sli1bSosWLaR///6yYMECASDBwcHy3XffiYjI7du3Zfbs2RISEiJubm7i5+cn0dHRkp6eLhs2bBCNRiMApEOHDpKVlSUbN24UnU4nAKRdu3byww8/yKVLlyQyMlJ8fHzE1dVV2rRpI0lJSXLnzp1HbsMax48fl6eeekoCAwMFgACQgIAAiYyMlC+//PKBn8vPzxcAkpSUZNX2RERGjRolo0aNsvpz91q3bp0EBAQIANFoNDJ8+HCLj62ISFxcnCiVSgkKChI3NzfR6XQyYsQIycrKMtlOQUGBDBo0SFQqlYSGhsqrr74qs2bNEgASEREh2dnZIiJy6tQpadeunajVaunfv79cv35d9u/fL56enrJkyZJ67atI3cYrOS/2Z+Nhi+8rqj+FCBfOcqTt27djzJgxXL/MAWJiYgAAO3bscFgMU6ZMwY4dO1BQUOCwGKzB8dq0sD8bD2f4viJeniNyuJrJJ4mIyLkxaaIGceHCBZNpCB70io2NdXSoZEcHDx7EnDlzsGvXLoSFhRnHwcsvv2xWd8iQIfD09ISrqyu6du2KU6dOOSBiyy1ZsqTWMX7vPGb3qq6uRnJyMiIjI2t9f9GiRejSpQt0Oh08PDwQERGBN954A7f+f3v3GtPk+f4B/FtoS1tsAcdxAgrIdEPQeYqghi1LtqFxiugk02RqRpAdGE4NosgQ8AgB40Jj3AwmuiAoBByKWzaDvpAZk4kiZjBRUEDkMLCWM/T6v/BPf9aCa5G2KNcn4QVP7+e+7j5XUy6ew32r1do2Z8+exYEDByxWeL/O+Rz0X3kCnk6HsnDhQshkMri5uSE2NlbnCWdL54mNIoteHGR8T4EFWfoegbi4OBKLxQSApkyZQqdPn7bYWAz1Mp/XhIQEWrZsGalUKu02Hx8feuONNwgAFRUV6e1TXFxMy5cvH/F4zSk5OVl7P96zP35+fnptq6qqaOHChQSAZs6cOWR/wcHBlJmZSa2traRSqSgnJ4dEIhF9/PHHOu0OHTpEwcHB1NbWZvSYOZ8vZkiebt26RVKplHbt2kVqtZquXLlCjo6OtGHDBp12L5MnIst/X7Gn+EwTYxayd+9e9PT0gIhw7949rFq1ytJDMpn9+/fj1KlTyM3NhVwu13nt8OHDsLKyQmRkJB4/fmyhEY6OEydO6E2LcevWLZ02N27cwPbt2xEVFYVZs2YN29eECRMQGRmJiRMnQi6X49NPP0VoaCguXLigXZcQAL799lvMnDkTS5YsQX9/v8ne27PGQz4NzVNycjJcXV2xe/du2NraIjAwELGxsTh+/LjOrP6WyBMbfVw0McZM6s6dO9i1axd2796tnZz1WUFBQYiJiUF9fT22bt1qgRGa18yZM5GXl4e1a9fCxsZm2HZFRUWwtrbW2ebo6AgAejPuJyYmoqysDIcOHRr9AT9nvOTTkDz19/fj3LlzCA4OhkAg0G4PCQkBEaGwsFCnvTnzxEyDiybGmEkdPnwYRIRPPvlk2DYpKSl466238NNPP+H3339/YX9EhPT0dO0ixw4ODlixYoXOf/VKpRK2traQyWQoLCxESEgIFAoF3N3dkZ2drdPfwMAAEhIS4OnpCalUioCAAOTk5LzcmzaR+vp6SKVSeHl56Wx3cHBAcHAwDh06ZPIn4Tif/3P37l2o1Wp4enrqbPfx8QEA3Lx5U2e7OfPETIOLJsaYSZ07dw7Tpk2DTCYbto1UKsXx48dhZWWFiIgI7bqGQ0lMTERcXBx27tyJpqYmXL58GQ8ePMDixYvx6NEjAMCXX36JzZs3o6urC3K5HDk5Oaiuroa3tzciIiJ0Zlffvn07Dh48iIyMDDx8+BDLli3DZ599NqIleeLi4uDg4ACxWAwvLy+sWLEC165dM7qfoXR2duLixYuIiIjQLv78rHfffRf19fW4cePGqMQbznjK539pbGwEAL1LlBKJBFKpVDv+Z5krT8w0uGhijJlMR0cH7t27p/3P+0UCAwOxefNm1NTUYPv27UO26erqQnp6OlauXIl169bBzs4O/v7+OHLkCFpaWvRmxgeeXi5SKBRwcnJCeHg4Ojo6cP/+fQBAd3c3lEolQkNDERYWBnt7e8THx0MkEiErK8uo9/r555/j7NmzePDgAdRqNbKzs3H//n0EBwejoqLCqL6GsnfvXri5uSElJWXI1319fQEA5eXlLx1rOOMpn4YYfELu+cuoACASidDV1aW33Rx5YqYjtPQA2FODE5cx8/nzzz8B8LE3Rl1dnVHtm5qaQEQvPCvxrJSUFBQVFSEzMxNr1qzRe72iogJqtVpvjcF58+ZBLBbj6tWrL+x/8AzN4JmJyspKdHZ26kwLIJVK4erqqnN5yBAeHh7w8PDQ/r5gwQJkZWVh1qxZyMzMhFKpNKq/Z+Xn5yM3Nxe//fab3lmNQYPHeKizG6NlPOXTEIP3dA11Y3dvby+kUqnednPkiZkOn2lijJlMd3c3ALzwhudnSSQSZGVlQSAQYOPGjXr/qbe3twN4+mTZ8+zt7fHkyROjxjd42Sg+Pl5nbqXa2lq9m61Hwt/fH9bW1qiqqhpxH6dOncL+/ftRUlKCKVOmDNtu8A/04DE3hfGez+e5uroCAFQqlc72zs5OdHd3w83NTW8fc+SJmQ6faRojeGp88+NlCYw3uOyGoQb/QBgzqV9gYCC+++47pKWlITk5WecmW3t7ewAY8o9pe3s73N3dDY4DPF0AGwAyMjIQExNj1L6G0Gg00Gg0BhcZz/vhhx/w66+/4uLFi0MWFs/q7e0FgCHPboyW8Z7P53l5eUEul+stnn3nzh0AQEBAgN4+5sgTMx0+08QYMxlnZ2cIBAKj5+tJTk7G9OnTcf36dZ3tM2bMwIQJE/Ru6r169Sp6e3sxZ84co+J4eHhAIpGgrKzMqP2G8tFHH+ltu3btGogIgYGBRvVFRIiNjUV5eTkKCgr+s2ACoD3GLi4uRsUyxnjKpyGEQiGWLFmCy5cvQ6PRaLcXFxdDIBAM+YShOfLETIeLJsaYychkMnh7ext9L9TgZZ3nb7CVSCTYsmUL8vPzcfLkSahUKpSXlyMqKgpubm6IjIw0Os6GDRuQnZ0NpVIJlUqFgYEB1NXV4eHDhwCA8PBwuLi4/OeyH/X19Th16hTa29vR19eH0tJSfPHFF/D09ERUVJRR47p9+zYOHjyIH3/8ESKRSG9plrS0NL19Bo+xv7+/UbGMMZ7yaahdu3bh0aNH+P7779HR0YHS0lKkpqZi/fr1mDZtml57c+SJmZD5JyFnz+JlVCyHlyUw3kg+r9HR0SQSiaizs1O7LT8/n3x8fAgAOTo60tdffz3kvtu2bdNbdkOj0VBqair5+vqSSCQiBwcHCg0NpcrKSm2bzMxMkslkBIB8fX2purqajh49SgqFggDQ5MmTqaqqioiIenp6KDY2ljw9PUkoFJKTkxOFhYVRRUUFERGFhoYSAEpISHjh+9yyZQv5+PiQra0tCYVCcnd3p4iICGpoaNBpV1paSgsXLiQ3NzftUiuurq4UFBREly5dIiKi8vLyIZdkGfxJTU3Vi7906VKaNGkSaTSaF47zWZzP4RmSp0GXLl2i+fPnk42NDbm5udG2bduou7t7yH5Hkici/r4aK/ivtYVx0WQ5/CVkvJF8Xv/55x8SCoV04sQJE43KtAYGBmjx4sV07NgxSw9lWC0tLSSRSCgtLc2o/Tif5jXSPBHx99VYwZfnGGMmNXXqVCQlJSEpKQlqtdrSwzHKwMAACgoK8OTJE4SHh1t6OMNKTEzErFmzEB0dbfJYnM+RM2eemGlw0TQO5OXlwdvbW+++CLFYDGdnZ7z33ntITU1FW1ubpYfKXlNxcXFYvXo1wsPDX6lFXEtKSpCXl4fi4mKD5yYyt/T0dJSVleH8+fMQiURmicn5NJ4l8sRGHxdN40BYWBju3r0LHx8f2NnZgYig0WjQ1NSE3NxceHl5ITY2Fn5+fiZZaoAxANizZw+io6Oxb98+Sw/FYB988AF+/vln7Xw8Y01hYSF6enpQUlICBwcHs8bmfBrOknlio4uLpnFKIBDA3t4e7733HrKyspCbm4tHjx5h6dKlr9R/jq+6rq4uBAUFvfIxDPXhhx9i//79lh7Ga2P58uWIi4sbchkPc+B8GsbSeWKjh4smBgBYtWoV1q9fj6amJhw5csTSwxk3jh07hqamplc+BmOMjQdcNDGt9evXA3g6MduggYEBJCQkwNPTE1KpFAEBAcjJyQEAKJVK2NraQiaTobCwECEhIVAoFHB3d0d2drZO35cuXcL8+fMhk8mgUCjg7++vXXrgRTHGGiJCeno63n77bdjY2MDBwQErVqzQWdcqOjoaYrFY5xLAV199BVtbWwgEArS0tAAAYmJisGXLFlRXV0MgEGDq1Kk4fPgwJBIJnJ2dsWnTJri5uUEikSAoKEhnHa6XiQEAFy5cgEKhwJ49e0x6vBhj7LVi6cf3xjtzTjng4+NDdnZ2w76uUqkIAHl4eGi3bd26lWxsbOjMmTPU1tZGO3bsICsrK7p27RoREe3cuZMA0B9//EGPHz+mpqYmWrx4Mdna2lJvby8REanValIoFHTgwAHq6uqixsZGWrlyJTU3NxsUw1RG8ghvQkICicViOnHiBLW3t9PNmzdp9uzZ5OjoSI2Njdp2a9euJRcXF519U1NTCYD2fRMRhYWFkY+Pj067yMhIsrW1pdu3b1N3dzdVVFTQvHnzSC6X0/3790clRlFREcnlckpKSjLq/fMUGa8Xzuerg6ccGBv4TBPTksvlEAgE2nWguru7oVQqERoairCwMNjb2yM+Ph4ikQhZWVk6+wYFBUGhUMDJyQnh4eHo6OjA/fv3AQA1NTVQqVTw8/ODRCKBi4sL8vLy4OjoaFQMS+vq6kJ6ejpWrlyJdevWwc7ODv7+/jhy5AhaWlpw9OjRUYslFAq1Z7PeeecdKJVKPHnyZNSOydKlS6FSqbBr165R6Y8xxsYDLpqYVkdHB4gICoUCAFBZWYnOzk7MmDFD20YqlcLV1VXnctTzxGIxAKCvrw8A4O3tDWdnZ6xbtw6JiYmoqanRth1pDEuoqKiAWq3G3LlzdbbPmzcPYrFY5/LZaJs7dy5kMtmYOyaMMTaecNHEtKqqqgAA06dPB/C0iAKA+Ph4nfmdamtr0dnZaXC/UqkUFy9exKJFi7Bnzx54e3sjPDwcXV1doxbDHNrb2wFgyMVT7e3th1ypfTTZ2NigubnZpDEYY4wNj4smpnXhwgUAQEhICADAyckJAJCRkQF6uuSO9qe0tNSovv38/PDLL7+goaEBsbGxyMnJQVpa2qjGMDV7e3sAGLI4am9vh7u7u8li9/X1mTwGY4yxF+OiiQEAGhsbkZGRAXd3d2zcuBEA4OHhAYlEgrKyspfqu6GhAbdv3wbwtBDbt28fZs+ejdu3b49aDHOYMWMGJkyYoDcB6NWrV9Hb24s5c+ZotwmFQu3lydFQUlICIsKCBQtMFoMxxtiLcdE0zhAR1Go1NBoNiAjNzc3IycnBwoULYW1tjYKCAu09TRKJBBs2bEB2djaUSiVUKhUGBgZQV1eHhw8fGhyzoaEBmzZtwt9//43e3l5cv34dtbW1WLBgwajFMAeJRIItW7YgPz8fJ0+ehEqlQnl5OaKiouDm5obIyEht26lTp+Lff/9FQUEB+vr60NzcjNraWr0+J06ciIaGBtTU1ODJkyfaIkij0aCtrQ39/f24efMmYmJi4OnpqZ0W4mVjFBcX85QDjDFmLMs8tMcGmeOR37Nnz1JAQADJZDISi8VkZWVFAEggEJC9vT3Nnz+fkpKSqLW1VW/fnp4eio2NJU9PTxIKheTk5ERhYWFUUVFBmZmZJJPJCAD5+vpSdXU1HT16lBQKBQGgyZMnU1VVFdXU1FBQUBA5ODiQtbU1vfnmm7Rz507q7+//zximNJJHeDUaDaWmppKvry+JRCJycHCg0NBQqqys1GnX2tpK77//PkkkEvLy8qJvvvmGtm3bRgBo6tSp2qkD/vrrL5o8eTJJpVJatGgRNTY2UmRkJIlEIpo0aRIJhUJSKBS0YsUKqq6uHrUY58+fJ7lcTikpKUa9f35E/fXC+Xx18JQDY4OAiMhyJRvLzc3FmjVrwGkwv9WrVwMATp8+beGR6Nq0aRNOnz6N1tZWSw9FD39eXy+cz1fHWP2+Gm/48hxjY9DAwIClh8AYY+w5XDQxxhhjjBmAiybGxpAdO3YgKysLjx8/hpeXF86cOWPpITHGGPt/QksPgDH2P3v37sXevXstPQzGGGND4DNNjDHGGGMG4KKJMcYYY8wAXDQxxhhjjBmAiybGGGOMMQNw0cQYY4wxZgB+em6MEAgElh7CuMXH3nh8zF4vnM9Xw6pVqyw9hHGPl1GxsLq6Oly5csXSw2CMMTbGeXh4IDAw0NLDGNe4aGKMMcYYMwDf08QYY4wxZgAumhhjjDHGDMBFE2OMMcaYAYQATlt6EIwxxhhjY93/ASpdsa+VO0+sAAAAAElFTkSuQmCC\n"
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multimodal_model.layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EkMs6TyHm8j",
        "outputId": "0d3d059a-bb73-48f4-8255-6fc733494481"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f13ac1f8d50>,\n",
              " <keras.engine.functional.Functional at 0x7f13ac154210>,\n",
              " <keras.layers.core.dense.Dense at 0x7f13ac161b10>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_fine = multimodal_model.fit(train_ds, validation_data=val_ds, epochs=50, verbose=2)\n",
        "\n",
        "# y_pred = multimodal_model.predict(test_ds)\n",
        "# y_pred = [np.argmax(i) for i in y_pred]\n",
        "\n",
        "# y_true = []\n",
        "# for element in test_ds:\n",
        "#     y_true.extend(list(element[1].numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlqvssvEHm-Q",
        "outputId": "eb2ce5e4-8dc1-4b2e-aab3-329a066a02b5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text_inputs'] which did not match any model input. They will be ignored by the model.\n",
            "  inputs = self._flatten_to_reference_inputs(inputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 - 16s - loss: 0.6234 - accuracy: 0.7910 - val_loss: 0.9093 - val_accuracy: 0.7077 - 16s/epoch - 370ms/step\n",
            "Epoch 2/50\n",
            "44/44 - 16s - loss: 0.5954 - accuracy: 0.8101 - val_loss: 0.9095 - val_accuracy: 0.7077 - 16s/epoch - 354ms/step\n",
            "Epoch 3/50\n",
            "44/44 - 16s - loss: 0.5854 - accuracy: 0.8133 - val_loss: 0.9098 - val_accuracy: 0.7106 - 16s/epoch - 354ms/step\n",
            "Epoch 4/50\n",
            "44/44 - 15s - loss: 0.5789 - accuracy: 0.8169 - val_loss: 0.9102 - val_accuracy: 0.7106 - 15s/epoch - 345ms/step\n",
            "Epoch 5/50\n",
            "44/44 - 15s - loss: 0.5726 - accuracy: 0.8176 - val_loss: 0.9110 - val_accuracy: 0.7135 - 15s/epoch - 341ms/step\n",
            "Epoch 6/50\n",
            "44/44 - 15s - loss: 0.5661 - accuracy: 0.8190 - val_loss: 0.9117 - val_accuracy: 0.7163 - 15s/epoch - 342ms/step\n",
            "Epoch 7/50\n",
            "44/44 - 15s - loss: 0.5597 - accuracy: 0.8208 - val_loss: 0.9117 - val_accuracy: 0.7163 - 15s/epoch - 352ms/step\n",
            "Epoch 8/50\n",
            "44/44 - 15s - loss: 0.5532 - accuracy: 0.8223 - val_loss: 0.9117 - val_accuracy: 0.7135 - 15s/epoch - 352ms/step\n",
            "Epoch 9/50\n",
            "44/44 - 15s - loss: 0.5467 - accuracy: 0.8259 - val_loss: 0.9122 - val_accuracy: 0.7106 - 15s/epoch - 343ms/step\n",
            "Epoch 10/50\n",
            "44/44 - 15s - loss: 0.5402 - accuracy: 0.8273 - val_loss: 0.9127 - val_accuracy: 0.7106 - 15s/epoch - 340ms/step\n",
            "Epoch 11/50\n",
            "44/44 - 15s - loss: 0.5337 - accuracy: 0.8309 - val_loss: 0.9133 - val_accuracy: 0.7049 - 15s/epoch - 343ms/step\n",
            "Epoch 12/50\n",
            "44/44 - 15s - loss: 0.5273 - accuracy: 0.8338 - val_loss: 0.9146 - val_accuracy: 0.7077 - 15s/epoch - 352ms/step\n",
            "Epoch 13/50\n",
            "44/44 - 15s - loss: 0.5212 - accuracy: 0.8373 - val_loss: 0.9146 - val_accuracy: 0.7049 - 15s/epoch - 345ms/step\n",
            "Epoch 14/50\n",
            "44/44 - 15s - loss: 0.5148 - accuracy: 0.8399 - val_loss: 0.9156 - val_accuracy: 0.7049 - 15s/epoch - 343ms/step\n",
            "Epoch 15/50\n",
            "44/44 - 15s - loss: 0.5086 - accuracy: 0.8420 - val_loss: 0.9169 - val_accuracy: 0.7077 - 15s/epoch - 346ms/step\n",
            "Epoch 16/50\n",
            "44/44 - 16s - loss: 0.5026 - accuracy: 0.8442 - val_loss: 0.9177 - val_accuracy: 0.7077 - 16s/epoch - 353ms/step\n",
            "Epoch 17/50\n",
            "44/44 - 15s - loss: 0.4964 - accuracy: 0.8460 - val_loss: 0.9189 - val_accuracy: 0.7106 - 15s/epoch - 342ms/step\n",
            "Epoch 18/50\n",
            "44/44 - 15s - loss: 0.4905 - accuracy: 0.8470 - val_loss: 0.9199 - val_accuracy: 0.7135 - 15s/epoch - 343ms/step\n",
            "Epoch 19/50\n",
            "44/44 - 16s - loss: 0.4847 - accuracy: 0.8503 - val_loss: 0.9215 - val_accuracy: 0.7106 - 16s/epoch - 353ms/step\n",
            "Epoch 20/50\n",
            "44/44 - 15s - loss: 0.4788 - accuracy: 0.8503 - val_loss: 0.9228 - val_accuracy: 0.7106 - 15s/epoch - 343ms/step\n",
            "Epoch 21/50\n",
            "44/44 - 15s - loss: 0.4731 - accuracy: 0.8521 - val_loss: 0.9244 - val_accuracy: 0.7077 - 15s/epoch - 346ms/step\n",
            "Epoch 22/50\n",
            "44/44 - 15s - loss: 0.4673 - accuracy: 0.8542 - val_loss: 0.9267 - val_accuracy: 0.7135 - 15s/epoch - 343ms/step\n",
            "Epoch 23/50\n",
            "44/44 - 15s - loss: 0.4617 - accuracy: 0.8549 - val_loss: 0.9290 - val_accuracy: 0.7135 - 15s/epoch - 347ms/step\n",
            "Epoch 24/50\n",
            "44/44 - 15s - loss: 0.4560 - accuracy: 0.8575 - val_loss: 0.9303 - val_accuracy: 0.7163 - 15s/epoch - 350ms/step\n",
            "Epoch 25/50\n",
            "44/44 - 15s - loss: 0.4502 - accuracy: 0.8596 - val_loss: 0.9333 - val_accuracy: 0.7163 - 15s/epoch - 343ms/step\n",
            "Epoch 26/50\n",
            "44/44 - 15s - loss: 0.4449 - accuracy: 0.8621 - val_loss: 0.9354 - val_accuracy: 0.7163 - 15s/epoch - 348ms/step\n",
            "Epoch 27/50\n",
            "44/44 - 15s - loss: 0.4394 - accuracy: 0.8625 - val_loss: 0.9369 - val_accuracy: 0.7192 - 15s/epoch - 343ms/step\n",
            "Epoch 28/50\n",
            "44/44 - 15s - loss: 0.4340 - accuracy: 0.8650 - val_loss: 0.9405 - val_accuracy: 0.7192 - 15s/epoch - 345ms/step\n",
            "Epoch 29/50\n",
            "44/44 - 15s - loss: 0.4287 - accuracy: 0.8643 - val_loss: 0.9429 - val_accuracy: 0.7221 - 15s/epoch - 352ms/step\n",
            "Epoch 30/50\n",
            "44/44 - 15s - loss: 0.4236 - accuracy: 0.8668 - val_loss: 0.9463 - val_accuracy: 0.7221 - 15s/epoch - 346ms/step\n",
            "Epoch 31/50\n",
            "44/44 - 15s - loss: 0.4182 - accuracy: 0.8682 - val_loss: 0.9482 - val_accuracy: 0.7221 - 15s/epoch - 342ms/step\n",
            "Epoch 32/50\n",
            "44/44 - 15s - loss: 0.4130 - accuracy: 0.8704 - val_loss: 0.9513 - val_accuracy: 0.7221 - 15s/epoch - 343ms/step\n",
            "Epoch 33/50\n",
            "44/44 - 15s - loss: 0.4079 - accuracy: 0.8715 - val_loss: 0.9545 - val_accuracy: 0.7221 - 15s/epoch - 351ms/step\n",
            "Epoch 34/50\n",
            "44/44 - 15s - loss: 0.4029 - accuracy: 0.8743 - val_loss: 0.9576 - val_accuracy: 0.7192 - 15s/epoch - 344ms/step\n",
            "Epoch 35/50\n",
            "44/44 - 15s - loss: 0.3981 - accuracy: 0.8776 - val_loss: 0.9612 - val_accuracy: 0.7163 - 15s/epoch - 348ms/step\n",
            "Epoch 36/50\n",
            "44/44 - 15s - loss: 0.3931 - accuracy: 0.8797 - val_loss: 0.9643 - val_accuracy: 0.7192 - 15s/epoch - 343ms/step\n",
            "Epoch 37/50\n",
            "44/44 - 15s - loss: 0.3882 - accuracy: 0.8804 - val_loss: 0.9666 - val_accuracy: 0.7135 - 15s/epoch - 342ms/step\n",
            "Epoch 38/50\n",
            "44/44 - 15s - loss: 0.3832 - accuracy: 0.8819 - val_loss: 0.9703 - val_accuracy: 0.7163 - 15s/epoch - 342ms/step\n",
            "Epoch 39/50\n",
            "44/44 - 16s - loss: 0.3784 - accuracy: 0.8851 - val_loss: 0.9745 - val_accuracy: 0.7163 - 16s/epoch - 353ms/step\n",
            "Epoch 40/50\n",
            "44/44 - 15s - loss: 0.3736 - accuracy: 0.8851 - val_loss: 0.9771 - val_accuracy: 0.7135 - 15s/epoch - 351ms/step\n",
            "Epoch 41/50\n",
            "44/44 - 15s - loss: 0.3687 - accuracy: 0.8873 - val_loss: 0.9803 - val_accuracy: 0.7163 - 15s/epoch - 343ms/step\n",
            "Epoch 42/50\n",
            "44/44 - 15s - loss: 0.3642 - accuracy: 0.8876 - val_loss: 0.9828 - val_accuracy: 0.7135 - 15s/epoch - 344ms/step\n",
            "Epoch 43/50\n",
            "44/44 - 15s - loss: 0.3596 - accuracy: 0.8901 - val_loss: 0.9872 - val_accuracy: 0.7135 - 15s/epoch - 348ms/step\n",
            "Epoch 44/50\n",
            "44/44 - 15s - loss: 0.3552 - accuracy: 0.8912 - val_loss: 0.9915 - val_accuracy: 0.7135 - 15s/epoch - 343ms/step\n",
            "Epoch 45/50\n",
            "44/44 - 16s - loss: 0.3506 - accuracy: 0.8948 - val_loss: 0.9947 - val_accuracy: 0.7163 - 16s/epoch - 367ms/step\n",
            "Epoch 46/50\n",
            "44/44 - 15s - loss: 0.3462 - accuracy: 0.8977 - val_loss: 0.9982 - val_accuracy: 0.7192 - 15s/epoch - 343ms/step\n",
            "Epoch 47/50\n",
            "44/44 - 15s - loss: 0.3419 - accuracy: 0.8984 - val_loss: 1.0027 - val_accuracy: 0.7192 - 15s/epoch - 348ms/step\n",
            "Epoch 48/50\n",
            "44/44 - 15s - loss: 0.3375 - accuracy: 0.9009 - val_loss: 1.0078 - val_accuracy: 0.7163 - 15s/epoch - 346ms/step\n",
            "Epoch 49/50\n",
            "44/44 - 15s - loss: 0.3334 - accuracy: 0.9031 - val_loss: 1.0123 - val_accuracy: 0.7106 - 15s/epoch - 343ms/step\n",
            "Epoch 50/50\n",
            "44/44 - 15s - loss: 0.3291 - accuracy: 0.9045 - val_loss: 1.0178 - val_accuracy: 0.7135 - 15s/epoch - 345ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nd561cPlckll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}